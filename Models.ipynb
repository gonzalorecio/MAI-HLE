{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xml.dom.minidom import parse, parseString\n",
    "from nltk import tokenize as tk\n",
    "import nltk\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "\n",
    "class Question(object):\n",
    "    def __init__(self, q_id, q, a_ids, a, r, s, l):\n",
    "        self.question_id = q_id\n",
    "        self.question = q\n",
    "        self.answer_ids = a_ids\n",
    "        self.answers = a\n",
    "        self.reference_rank = r\n",
    "        self.system_rank = s\n",
    "        self.labels = l\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.question}\\n  {self.answers}\\n  {self.reference_rank}\\n  {self.system_rank}\\n  {self.labels}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "class QuestionsAndAnswers(list):\n",
    "    def __init__(self, dataset='Train'):\n",
    "        ''' dataset = {Train,Test,Validation} '''\n",
    "        list.__init__(self)\n",
    "        self.PATH = 'MEDIQA2019_datasets/MEDIQA_Task3_QA/'\n",
    "        p = self.read_dataset(dataset)\n",
    "        self.extend(self.read_dataset(dataset))\n",
    "        \n",
    "        self.references = [np.array(q.reference_rank) for q in self]\n",
    "        self.labels = [np.array(q.labels) for q in self]\n",
    "\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        s = unicodedata.normalize(\"NFKD\", text.lower())\n",
    "        return re.sub(r'\\[\\d\\]', '', s)\n",
    "\n",
    "    def get_answers(self, answers):\n",
    "        # return np.array((map(lambda ans: preprocess_text(ans.getElementsByTagName('AnswerText')[0].firstChild.nodeValue), answers)))\n",
    "        answs, answs_ids, rank, chiqa, y = [], [], [], [], []\n",
    "        for answer in answers:\n",
    "            ans = self.preprocess_text(answer.getElementsByTagName('AnswerText')[0].firstChild.nodeValue)\n",
    "            a_id = answer.getAttribute('AID')\n",
    "            reference = int(answer.getAttribute('ReferenceRank'))\n",
    "            system = int(answer.getAttribute('SystemRank'))\n",
    "            label = answer.getAttribute('ReferenceScore')\n",
    "            answs.append(ans); answs_ids.append(a_id); rank.append(reference); chiqa.append(system); y.append(int(label in ['3','4']))\n",
    "        return answs, answs_ids, rank, chiqa, y\n",
    "    \n",
    "    def get_system_ranks(self):\n",
    "          return [q.system_rank for q in self]\n",
    "        \n",
    "    def get_reference_ranks(self):\n",
    "          return [q.reference_rank for q in self]\n",
    "        \n",
    "    def get_labels(self):\n",
    "          return [q.labels for q in self]\n",
    "\n",
    "    def read_dataset(self, dataset='Train'):\n",
    "        i = 0\n",
    "        indx2id = []\n",
    "        QA, QA2 = [], []  # QA2 has also system ranks from ChiQA\n",
    "        if dataset == 'Test': dataset = 'TestSet-wLabels'\n",
    "        for filename in os.listdir(self.PATH):\n",
    "            if not filename.endswith('.xml') or dataset not in filename: continue\n",
    "            tree = parse(self.PATH + filename)\n",
    "            questions = tree.getElementsByTagName('Question')\n",
    "            for question in questions:\n",
    "                qelem = question.getElementsByTagName('QuestionText')\n",
    "                q, q_id = self.preprocess_text(qelem[0].firstChild.nodeValue), question.getAttribute('QID')\n",
    "                # print(q) # --> questions\n",
    "                answers = question.getElementsByTagName('Answer')\n",
    "                answers_list, a_ids, rank, system, labels = self.get_answers(answers)\n",
    "                QA.append([q,answers_list, rank, labels])\n",
    "                question = Question(q_id=q_id, q=q, a_ids=a_ids, a=answers_list, r=rank, s=system, l=labels)\n",
    "                # QA2.append([q,answers_list, rank, system, labels])\n",
    "                QA2.append(question)\n",
    "                indx2id.append(q_id); i+=1;\n",
    "                # break\n",
    "        return QA2\n",
    "\n",
    "    def output_predictions(self, predictions, labels, file=''):\n",
    "        assert len(predictions) == len(self)\n",
    "        print('question_id,answer_id,label')\n",
    "        with open(f'task3/sample_submission_round_2_{file}.csv', mode='w') as csv_file:\n",
    "            for i, p in enumerate(predictions):\n",
    "                q_id = QA[i].question_id\n",
    "                answers = QA[i].answer_ids\n",
    "                # order = np.array(a)[np.argsort(p)]\n",
    "                order = np.array(answers)[np.argsort(p)]\n",
    "                # order = np.array(answers)[np.array(p)-1]\n",
    "                lab = labels[i]\n",
    "                ordered_lab = np.array(lab)[np.argsort(p)]\n",
    "                if file == '':\n",
    "                    \n",
    "                    for a_id, l in zip(order,ordered_lab):\n",
    "                        print(f\"{q_id},{a_id},{int(l)}\")\n",
    "                else:\n",
    "                    for a_id, l in zip(order,ordered_lab):\n",
    "                        csv_file.write(f\"{q_id},{a_id},{int(l)}\\n\")\n",
    "            \n",
    "    def normalize_sequence(self, seq):\n",
    "        seq = np.array(seq)\n",
    "        a = np.argsort(seq)\n",
    "        seq[a] = list(range(1,len(seq)+1))\n",
    "        return seq\n",
    "\n",
    "    def accuracy(self, predictions):\n",
    "        preds = np.concatenate(predictions)\n",
    "        true  = np.concatenate(self.labels) \n",
    "        assert len(preds) == len(true), f\"{len(preds)}, {len(true)}\"\n",
    "        return accuracy_score(true, preds)\n",
    "\n",
    "    def precision(self, predictions):\n",
    "        precisions = []\n",
    "        num_answers = []\n",
    "        for i in range(len(predictions)):\n",
    "            labels = self.labels[i]\n",
    "            p = self.normalize_sequence([x for j,x in enumerate(predictions[i]) if labels[j]==1])\n",
    "            r = self.normalize_sequence([x for j,x in enumerate(self.references[i]) if labels[j]==1])\n",
    "            if len(p) == 0:\n",
    "                print(predictions[i])\n",
    "            correct = sum([a == b for a,b in zip(p, r)])\n",
    "            # for a,b in zip(p, r)\n",
    "            # num_answers.append(len(p))\n",
    "            precisions.append(correct/len(p))\n",
    "        return np.mean(precisions)\n",
    "        # return np.average(np.array(precisions), weights=num_answers)\n",
    "\n",
    "    def mean_spearmanr(self, predictions):\n",
    "        assert len(predictions) == len(self.references)\n",
    "        count, total = 0, 0\n",
    "        preds, refs = [], []\n",
    "        for i in range(len(predictions)):\n",
    "            labels = self.labels[i]\n",
    "            assert len(predictions[i]) == len(labels), f\"{predictions}, {labels}\"\n",
    "            p = [x for j,x in enumerate(predictions[i]) if labels[j]==1]\n",
    "            r = [x for j,x in enumerate(self.references[i]) if labels[j]==1]\n",
    "            preds += p; refs += r\n",
    "            if len(r) == 1:\n",
    "                total += 1\n",
    "                count += 1\n",
    "            elif len(r) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                total += 1\n",
    "                count += spearmanr(p, r)[0]\n",
    "        return spearmanr(preds, refs)[0]\n",
    "        # return count/total\n",
    "\n",
    "    def mean_reciprocal_rank(self, predicted):\n",
    "        rs = []\n",
    "        for k, (a, b) in enumerate(zip(predicted, self.references)):\n",
    "            res = np.array(a)[np.argsort(b)]\n",
    "            labels = QA[k].labels\n",
    "            res = [r if labels[i]==1 else 100 for i,r in enumerate(res)]\n",
    "            rs.append([int(i==min(res)) for i in res])  # sets 1 in first ranked answer\n",
    "        rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
    "        return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA = QuestionsAndAnswers(dataset = 'Test') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_ranks = QA.get_system_ranks()\n",
    "reference_ranks = [q.reference_rank for q in QA]\n",
    "labels = [q.labels for q in QA]\n",
    "system_labels = [np.ones(len(l)) for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA.output_predictions(reference_ranks, labels)\n",
    "# QA.output_predictions(system_ranks, system_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_id,answer_id,label\n"
     ]
    }
   ],
   "source": [
    "QA.output_predictions(system_ranks, system_labels, file='test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Task (Round-2) : 3\n",
      "Ground truth: task3/ground_truth_round_2.csv\n",
      "Submission file: task3/sample_submission_round_2_test2.csv\n",
      "{'score_acc': 0.5167118337850045, 'score_secondary_spearman': 0.3149635036496349, 'meta': {'MRR': 0.895, 'Precision': 0.5167118337850045}}\n"
     ]
    }
   ],
   "source": [
    "import evaluator\n",
    "\n",
    "for task in [3]:\n",
    "    print(\"Testing Task (Round-2) : {}\".format(task))\n",
    "    answer_file_path = \"task{}/ground_truth_round_2.csv\".format(task)\n",
    "    _client_payload = {}\n",
    "    _client_payload[\"submission_file_path\"] = \"task{}/sample_submission_round_2_test2.csv\".format(task)\n",
    "\n",
    "    # Instaiate a dummy context\n",
    "    _context = {}\n",
    "    # Instantiate an evaluator\n",
    "    aicrowd_evaluator = evaluator.MediqaEvaluator(answer_file_path, task=task, round=2)\n",
    "    # Evaluate\n",
    "    result = aicrowd_evaluator._evaluate(_client_payload, _context)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline\n",
    "\n",
    "Testing Task (Round-2) : 3\n",
    "{'score_acc': 0.5167118337850045, 'score_secondary_spearman': 0.3149635036496349, 'meta': {'MRR': 0.895, 'Precision': 0.5167118337850045}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BioBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dmis-lab/biobert-large-cased-v1.1\")\n",
    "model = BertModel.from_pretrained('dmis-lab/biobert-large-cased-v1.1',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_sentence_embedding(sentence):\n",
    "    marked_text = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    # print(len(hidden_states.shape))\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "\n",
    "    # Calculate the average of all n token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "\n",
    "    return sentence_embedding\n",
    "\n",
    "def get_full_sentence_embedding(sentence):\n",
    "    embeddings = []\n",
    "    e = 0\n",
    "    max_size = 1024#512\n",
    "    for i in range(int(len(sentence)/max_size)+1):\n",
    "#         print(i, max_size*(i+1), len(sentence)/max_size)\n",
    "        e = get_bert_sentence_embedding(sentence[i*max_size:max_size*(i+1)])\n",
    "#         print(e)\n",
    "        embeddings.append(e)\n",
    "    embedding = torch.mean(torch.stack(embeddings), dim=0)\n",
    "    print(embedding)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0133,  0.2374,  0.0653,  ..., -0.0812,  0.2875,  0.0144])\n",
      "tensor([0.1322, 0.2805, 0.0596,  ..., 0.1509, 0.1216, 0.2607])\n",
      "tensor([-0.0759,  0.2243,  0.0005,  ..., -0.0341,  0.0171,  0.0987])\n",
      "tensor([-0.0166,  0.0915,  0.0515,  ...,  0.1757,  0.0220,  0.1903])\n",
      "tensor([-0.0189,  0.0753,  0.0633,  ...,  0.0618,  0.0875,  0.1337])\n",
      "tensor([0.0053, 0.1166, 0.0481,  ..., 0.0850, 0.0591, 0.1093])\n",
      "tensor([-0.0180,  0.2393,  0.1346,  ...,  0.0838,  0.1778,  0.2939])\n",
      "tensor([ 0.0107,  0.2480,  0.1055,  ..., -0.1089,  0.2255,  0.2621])\n"
     ]
    }
   ],
   "source": [
    "K = 7\n",
    "q = get_bert_sentence_embedding(QA[K].question)\n",
    "ans = [get_full_sentence_embedding(a) for a in QA[K].answers] # 512 is the maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label,Rank,Similarity\n",
      "1 2 0.96265709400177\n",
      "0 8 0.9442784786224365\n",
      "1 1 0.9374319911003113\n",
      "0 7 0.940800130367279\n",
      "0 6 0.9403217434883118\n",
      "1 4 0.9442769289016724\n",
      "0 5 0.9455589056015015\n",
      "1 3 0.9560166001319885\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "print('Label,Rank,Similarity')\n",
    "for i,a in enumerate(ans):\n",
    "    sim = 1-cosine(q, a)\n",
    "    print(QA.labels[K][i], QA.references[K][i], sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BioELMo\n",
    "\n",
    "https://docs.allennlp.org/v1.0.0rc5/tutorials/how_to/elmo/\n",
    "\n",
    "https://github.com/allenai/allennlp/blob/main/allennlp/modules/elmo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install allennlp\n",
    "# ! pip install allennlp-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo\n",
    "elmo = Elmo(\n",
    "    options_file='https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json', \n",
    "    weight_file='https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5',\n",
    "    num_output_representations=3,\n",
    "    dropout=0\n",
    "    \n",
    ")\n",
    "\n",
    "bioelmo = Elmo(\n",
    "    options_file='bioelmo/biomed_elmo_options.json', \n",
    "    weight_file='bioelmo/biomed_elmo_weights.hdf5',\n",
    "    num_output_representations=3,\n",
    "    dropout=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['First', 'sentence', '.'], ['Another', '.'], [\"I\", \"ate\", \"a\", \"carrot\", \"for\", \"breakfast\"]]\n",
    "character_ids = batch_to_ids(sentences)\n",
    "embeddings = bioelmo(character_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elmo_embedding(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "#     print(tokens)\n",
    "    sentences = [tokens]\n",
    "    character_ids = batch_to_ids(sentences)\n",
    "    return bioelmo(character_ids)['elmo_representations'][2].mean(dim=0).mean(dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2065, -0.2165,  0.1309,  ...,  0.0792,  0.1241,  0.0947],\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['elmo_representations'][0].mean(dim=0).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "q = get_elmo_embedding(QA[K].question)\n",
    "ans = [get_elmo_embedding(a) for a in QA[K].answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label,Rank,Similarity\n",
      "0 4 0.7380754351615906\n",
      "1 1 0.7956207394599915\n",
      "1 2 0.7879546284675598\n",
      "0 5 0.7838222980499268\n",
      "1 3 0.714128315448761\n",
      "0 6 0.7684296369552612\n",
      "0 7 0.6464023590087891\n",
      "0 9 0.6179548501968384\n",
      "0 10 0.6289558410644531\n",
      "0 8 0.7213234305381775\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "print('Label,Rank,Similarity')\n",
    "for i,a in enumerate(ans):\n",
    "    sim = 1-cosine(q.detach(), a.detach())\n",
    "    print(QA.labels[K][i], QA.references[K][i], sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
