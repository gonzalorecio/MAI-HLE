{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xml.dom.minidom import parse, parseString\n",
    "from nltk import tokenize as tk\n",
    "import nltk\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "\n",
    "class Question(object):\n",
    "    def __init__(self, q_id, q, a_ids, a, r, s, l):\n",
    "        self.question_id = q_id\n",
    "        self.question = q\n",
    "        self.answer_ids = a_ids\n",
    "        self.answers = a\n",
    "        self.reference_rank = r\n",
    "        self.system_rank = s\n",
    "        self.labels = l\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.question}\\n  {self.answers}\\n  {self.reference_rank}\\n  {self.system_rank}\\n  {self.labels}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "class QuestionsAndAnswers(list):\n",
    "    def __init__(self, dataset='Train', load_external_data=False):\n",
    "        ''' dataset = {Train,Test,Validation} '''\n",
    "        list.__init__(self)\n",
    "        self.PATH = 'MEDIQA2019_datasets/MEDIQA_Task3_QA/'\n",
    "        p = self.read_dataset(dataset)\n",
    "        self.extend(self.read_dataset(dataset))\n",
    "        if load_external_data:\n",
    "            self.extend(self.read_external_dataset())\n",
    "        \n",
    "        self.references = [np.array(q.reference_rank) for q in self]\n",
    "        self.labels = [np.array(q.labels) for q in self]\n",
    "\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        s = unicodedata.normalize(\"NFKD\", text.lower())\n",
    "        return re.sub(r'\\[\\d\\]', '', s)\n",
    "\n",
    "    def get_answers(self, answers):\n",
    "        # return np.array((map(lambda ans: preprocess_text(ans.getElementsByTagName('AnswerText')[0].firstChild.nodeValue), answers)))\n",
    "        answs, answs_ids, rank, chiqa, y = [], [], [], [], []\n",
    "        for answer in answers:\n",
    "            ans = self.preprocess_text(answer.getElementsByTagName('AnswerText')[0].firstChild.nodeValue)\n",
    "            a_id = answer.getAttribute('AID')\n",
    "            reference = int(answer.getAttribute('ReferenceRank'))\n",
    "            system = int(answer.getAttribute('SystemRank'))\n",
    "            label = answer.getAttribute('ReferenceScore')\n",
    "            answs.append(ans); answs_ids.append(a_id); rank.append(reference); chiqa.append(system); y.append(int(label in ['3','4']))\n",
    "        return answs, answs_ids, rank, chiqa, y\n",
    "    \n",
    "    def get_system_ranks(self):\n",
    "          return [q.system_rank for q in self]\n",
    "        \n",
    "    def get_reference_ranks(self):\n",
    "          return [q.reference_rank for q in self]\n",
    "        \n",
    "    def get_labels(self):\n",
    "          return [q.labels for q in self]\n",
    "\n",
    "    def read_dataset(self, dataset='Train'):\n",
    "        i = 0\n",
    "        indx2id = []\n",
    "        QA, QA2 = [], []  # QA2 has also system ranks from ChiQA\n",
    "        if dataset == 'Test': dataset = 'TestSet-wLabels'\n",
    "        for filename in os.listdir(self.PATH):\n",
    "            if not filename.endswith('.xml') or dataset not in filename: continue\n",
    "            tree = parse(self.PATH + filename)\n",
    "            questions = tree.getElementsByTagName('Question')\n",
    "            for question in questions:\n",
    "                qelem = question.getElementsByTagName('QuestionText')\n",
    "                q, q_id = self.preprocess_text(qelem[0].firstChild.nodeValue), question.getAttribute('QID')\n",
    "                # print(q) # --> questions\n",
    "                answers = question.getElementsByTagName('Answer')\n",
    "                answers_list, a_ids, rank, system, labels = self.get_answers(answers)\n",
    "                QA.append([q,answers_list, rank, labels])\n",
    "                question = Question(q_id=q_id, q=q, a_ids=a_ids, a=answers_list, r=rank, s=system, l=labels)\n",
    "                # QA2.append([q,answers_list, rank, system, labels])\n",
    "                QA2.append(question)\n",
    "                indx2id.append(q_id); i+=1;\n",
    "                # break\n",
    "        return QA2\n",
    "    \n",
    "    def read_external_dataset(self):\n",
    "        QA = []\n",
    "        PATH_EXTRA = 'MedQuAD/'\n",
    "        for filename in os.listdir(PATH_EXTRA + '/'):\n",
    "            if any(s in filename for s in ('CDC', 'SeniorHealth', 'GARD', 'GHR', 'NIDDK', 'NINDS', 'NHLBI', 'CancerGov')): # CDC, SeniorHealth, GARD,\n",
    "                dirname = PATH_EXTRA + '/' + filename\n",
    "                for file in os.listdir(dirname):\n",
    "                    fullname = dirname + '/' + file\n",
    "                    tree = parse(fullname)\n",
    "                    questions = tree.getElementsByTagName('QAPair')\n",
    "                    Q, QT, = [], []; QTypes = {}\n",
    "                    for question in questions:\n",
    "                        qelem = question.getElementsByTagName('Question')\n",
    "                        q, qid = self.preprocess_text(qelem[0].firstChild.nodeValue), question.getAttribute('qid')\n",
    "                        qtype = qelem[0].getAttribute('qtype')\n",
    "                        if question.getElementsByTagName('Answer')[0].firstChild is None: continue\n",
    "                        a = self.preprocess_text(question.getElementsByTagName('Answer')[0].firstChild.nodeValue)\n",
    "                        if qtype not in QTypes: \n",
    "                            QTypes[qtype] = {'q': q, 'a': [a]}\n",
    "                        else: \n",
    "                            QTypes[qtype]['a'].append(a)\n",
    "                        Q.append(q); QT.append(q + qtype)\n",
    "\n",
    "                    assert len(set(Q)) == len(set(QT)), 'Error reading MedQuAD dataset'\n",
    "                    for qtype in QTypes:\n",
    "                        q = QTypes[qtype]['q']\n",
    "                        # positive examples\n",
    "                        ans = QTypes[qtype]['a']\n",
    "                        question = Question(q_id=-1, q=q, a_ids=-1, a=ans, \n",
    "                                            r=[1]*len(ans), \n",
    "                                            s=[], l=[1]*len(ans))\n",
    "                        QA.append(question)\n",
    "                        # negative examples\n",
    "                        for qtype_other in QTypes:\n",
    "                            if qtype_other != qtype:\n",
    "                                ans_wrong = QTypes[qtype_other]['a']\n",
    "                                question = Question(q_id=-1, q=q, a_ids=[], a=ans_wrong, \n",
    "                                                    r=[int(len(ans))+2]*len(ans_wrong), \n",
    "                                                    s=[], l=[0]*len(ans_wrong))\n",
    "                                QA.append(question)\n",
    "                                \n",
    "        return QA\n",
    "\n",
    "    def output_predictions(self, predictions, labels, file=''):\n",
    "        assert len(predictions) == len(self)\n",
    "        print('question_id,answer_id,label')\n",
    "        with open(f'task3/sample_submission_round_2_{file}.csv', mode='w') as csv_file:\n",
    "            for i, p in enumerate(predictions):\n",
    "                q_id = self[i].question_id\n",
    "                answers = self[i].answer_ids\n",
    "                assert len(p) == len(answers), f'{len(p)} != {len(answers)}'\n",
    "                # order = np.array(a)[np.argsort(p)]\n",
    "                p = self.normalize_sequence(p)\n",
    "                order = np.array(answers)[np.argsort(p)]\n",
    "                # order = np.array(answers)[np.array(p)-1]\n",
    "                lab = labels[i]\n",
    "                ordered_lab = np.array(lab)[np.argsort(p)]\n",
    "                if file == '':\n",
    "                    \n",
    "                    for a_id, l in zip(order,ordered_lab):\n",
    "                        print(f\"{q_id},{a_id},{int(l)}\")\n",
    "                else:\n",
    "                    for a_id, l in zip(order,ordered_lab):\n",
    "                        csv_file.write(f\"{q_id},{a_id},{int(l)}\\n\")\n",
    "            \n",
    "    def normalize_sequence(self, seq):\n",
    "        seq = np.array(seq)\n",
    "        a = np.argsort(seq)\n",
    "        seq[a] = list(range(1,len(seq)+1))\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA = QuestionsAndAnswers(dataset = 'Train') \n",
    "QA_extended = QuestionsAndAnswers(dataset = 'Train', load_external_data=True) \n",
    "QA_val = QuestionsAndAnswers(dataset = 'Validation')\n",
    "QA_test = QuestionsAndAnswers(dataset = 'Test') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 65392, 25, 150)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(QA), len(QA_extended), len(QA_val), len(QA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_ranks = QA_test.get_system_ranks()\n",
    "reference_ranks = [q.reference_rank for q in QA_test]\n",
    "labels = [q.labels for q in QA_test]\n",
    "system_labels = [np.ones(len(l)) for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA.output_predictions(reference_ranks, labels)\n",
    "# QA.output_predictions(system_ranks, system_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_id,answer_id,label\n"
     ]
    }
   ],
   "source": [
    "QA_test.output_predictions(system_ranks, system_labels, file='test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Task (Round-2) : 3\n",
      "Ground truth: task3/ground_truth_round_2.csv\n",
      "Submission file: task3/sample_submission_round_2_test2.csv\n",
      "{'score_acc': 0.5167118337850045, 'score_secondary_spearman': 0.3149635036496349, 'meta': {'MRR': 0.895, 'Precision': 0.5167118337850045}}\n"
     ]
    }
   ],
   "source": [
    "import evaluator\n",
    "\n",
    "def evaluate(filename):\n",
    "    for task in [3]:\n",
    "        print(f\"Testing Task (Round-2) : {task}\")\n",
    "        answer_file_path = f\"task{task}/ground_truth_round_2.csv\"\n",
    "        _client_payload = {}\n",
    "        _client_payload[\"submission_file_path\"] = f\"task{task}/sample_submission_round_2_{filename}.csv\"\n",
    "\n",
    "        # Instaiate a dummy context\n",
    "        _context = {}\n",
    "        # Instantiate an evaluator\n",
    "        aicrowd_evaluator = evaluator.MediqaEvaluator(answer_file_path, task=task, round=2)\n",
    "        # Evaluate\n",
    "        result = aicrowd_evaluator._evaluate(_client_payload, _context)\n",
    "        print(result)\n",
    "\n",
    "evaluate('test2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline\n",
    "\n",
    "Testing Task (Round-2) : 3\n",
    "{'score_acc': 0.5167118337850045, 'score_secondary_spearman': 0.3149635036496349, 'meta': {'MRR': 0.895, 'Precision': 0.5167118337850045}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BioBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "\n",
    "model_name = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "# model_name = 'amoux/scibert_nli_squad'\n",
    "# model_name = 'bert-base-uncased'\n",
    "# model_name = 'sentence-transformers/distilbert-base-nli-stsb-mean-tokens'\n",
    "# model_name = 'sentence-transformers/roberta-base-nli-stsb-mean-tokens'\n",
    "# model_name = 'dmis-lab/biobert-base-cased-v1.1-squad'\n",
    "# model_name = 'dmis-lab/biobert-v1.1'\n",
    "# model_name = 'dmis-lab/biobert-large-cased-v1.1'\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"dmis-lab/biobert-large-cased-v1.1\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# model = BertModel.from_pretrained('dmis-lab/biobert-large-cased-v1.1',\n",
    "#                                   output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "#                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_bert_sentence_embedding(sentence):\n",
    "    marked_text = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        CLS = outputs[0][0]\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    # print(len(hidden_states.shape))\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "\n",
    "    # Calculate the average of all n token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "\n",
    "    return sentence_embedding\n",
    "\n",
    "def get_CLS(sentence):\n",
    "    marked_text = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "    # Map the token strings to their vocabulary indeces.\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    CLS = outputs[0][:,0,:]\n",
    "    return CLS\n",
    "\n",
    "def get_full_sentence_embedding(sentence):\n",
    "    embeddings = []\n",
    "    e = 0\n",
    "    max_size = 1024#512\n",
    "    for i in range(int(len(sentence)/max_size)+1):\n",
    "#         print(i, max_size*(i+1), len(sentence)/max_size)\n",
    "#         e = get_bert_sentence_embedding(sentence[i*max_size:max_size*(i+1)])\n",
    "        e = get_CLS(sentence[i*max_size:max_size*(i+1)])\n",
    "#         print(e)\n",
    "        embeddings.append(e)\n",
    "    embedding = torch.mean(torch.stack(embeddings), dim=0)\n",
    "    print(embedding)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten   = lambda t: [item for sublist in t for item in sublist]\n",
    "\n",
    "sentences = [[q.question, a]  for q in QA for a in q.answers]\n",
    "labels    = flatten([q.labels for q in QA])\n",
    "rankings  = flatten([q.reference_rank for q in QA])\n",
    "\n",
    "sentences_extended = [[q.question, a]  for q in QA_extended for a in q.answers]\n",
    "labels_extended    = flatten([q.labels for q in QA_extended])\n",
    "rankings_extended  = flatten([q.reference_rank for q in QA_extended])\n",
    "\n",
    "sentences_val = [[q.question, a]  for q in QA_val for a in q.answers]\n",
    "labels_val    = flatten([q.labels for q in QA_val]) \n",
    "rankings_val  = flatten([q.reference_rank for q in QA_val])\n",
    "\n",
    "sentences_test = [[q.question, a]  for q in QA_test for a in q.answers]\n",
    "labels_test    = flatten([q.labels for q in QA_test])\n",
    "rankings_test  = flatten([q.reference_rank for q in QA_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71960"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "BATCH_SIZE_TEST = 32#64\n",
    "max_len_seq = 512\n",
    "\n",
    "class MEDIQA_Dataset(Dataset):\n",
    "    def __init__(self, X, y, r, transform=None):\n",
    "        self.X = []\n",
    "        self.y = np.array(y)\n",
    "        self.r = np.array(r)\n",
    "        for q, a in X:\n",
    "            _q =  tokenizer.encode_plus(\n",
    "                    text=q,  # the sentence to be encoded\n",
    "                    add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "                    max_length = 512,  # maximum length of a sentence\n",
    "                    truncation=True,\n",
    "                    padding='max_length',  # Add [PAD]s\n",
    "                    return_attention_mask = True,  # Generate the attention mask\n",
    "#                     return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    "                )\n",
    "            _a =  tokenizer.encode_plus(\n",
    "                    text=a,  # the sentence to be encoded\n",
    "                    add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "                    truncation=True,\n",
    "                    max_length = 512,  # maximum length of a sentence\n",
    "                    padding='max_length',  # Add [PAD]s\n",
    "                    return_attention_mask = True,  # Generate the attention mask\n",
    "#                     return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
    "                )\n",
    "#             _q = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"[CLS] \" + q + \" [SEP]\"))[:max_len_seq]\n",
    "#             _q += [0]*(max_len_seq-len(_q))\n",
    "#             _a = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"[CLS] \" + a + \" [SEP]\"))[:max_len_seq]\n",
    "#             _a += [0]*(max_len_seq-len(_a))\n",
    "            self.X.append([_q['input_ids'], _q['attention_mask'],\n",
    "                           _a['input_ids'], _a['attention_mask']])\n",
    "        self.X = np.array(self.X)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        score = torch.FloatTensor([self.y[index]])\n",
    "        rank  = torch.FloatTensor([self.r[index]])\n",
    "        q = torch.LongTensor(self.X[index][0])\n",
    "        q_att = torch.LongTensor(self.X[index][1])\n",
    "        a = torch.LongTensor(self.X[index][2])\n",
    "        a_att = torch.LongTensor(self.X[index][3])\n",
    "        \n",
    "        return score, rank, q, q_att, a, a_att\n",
    "\n",
    "# Create train dataset\n",
    "train_dataset = MEDIQA_Dataset(X=sentences, y=labels, r=rankings)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "LIMIT = 6000000\n",
    "train_dataset_extended = MEDIQA_Dataset(X=sentences_extended[:LIMIT], y=labels_extended[:LIMIT], r=rankings_extended[:LIMIT])\n",
    "train_loader_extended = DataLoader(train_dataset_extended, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Create validation dataset\n",
    "val_dataset = MEDIQA_Dataset(X=sentences_val, y=labels_val, r=rankings_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# Create test dataset\n",
    "test_dataset = MEDIQA_Dataset(X=sentences_test, y=labels_test, r=rankings_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE_TEST, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Residual\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MEDIQA_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MEDIQA_Model, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        # self.bert_q = AutoModel.from_pretrained(model_name)\n",
    "        # self.bert_a = AutoModel.from_pretrained(model_name)\n",
    "        modules = [self.bert.embeddings, *self.bert.encoder.layer[:-3],]\n",
    "        # modules = [self.bert_q.embeddings, *self.bert_q.encoder.layer[:-1],\n",
    "        #            self.bert_a.embeddings, *self.bert_a.encoder.layer[:-1]] #Replace 5 by what you want\n",
    "        for module in modules:\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.linear1 = nn.Linear(2*self.bert.config.hidden_size, 1024)\n",
    "        self.linear2 = nn.Linear(1024, 2*self.bert.config.hidden_size)\n",
    "        self.linear3 = nn.Linear(2*self.bert.config.hidden_size, 256)\n",
    "        self.linear4 = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "\n",
    "    def forward(self, q, q_att, a, a_att):\n",
    "        # _, pooled_output = self.bert(tokens, output_all=False)\n",
    "        # print(q.shape, a.shape)\n",
    "        # q_emb = self.get_CLS(self.bert, q, q_att)\n",
    "        # a_emb = self.get_CLS(self.bert, a, a_att)\n",
    "        # q_emb = self.mean_pooling(self.bert, q, q_att)\n",
    "        # a_emb = self.mean_pooling(self.bert, a, a_att)\n",
    "        q_emb = self.max_pooling(self.bert, q, q_att)\n",
    "        a_emb = self.max_pooling(self.bert, a, a_att)\n",
    "        # q_emb = self.get_hidden_state_average(q, q_att)\n",
    "        # a_emb = self.get_hidden_state_average(a, a_att)\n",
    "        # print('CLS:', q_emb.shape, a_emb.shape)\n",
    "        concat_emb = torch.cat([q_emb, a_emb], dim=1)\n",
    "        # print('concat:', x.shape, x)\n",
    "        residual = concat_emb\n",
    "        x = self.linear1(concat_emb)\n",
    "        x = self.dropout1(x)\n",
    "        x = nn.LeakyReLU(0.1)(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = nn.LeakyReLU(0.1)(x)\n",
    "        \n",
    "        x += residual\n",
    "        \n",
    "        x = self.linear3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = nn.LeakyReLU(0.1)(x)\n",
    "        \n",
    "        x = self.linear4(x)\n",
    "        prob = self.sigmoid(x)\n",
    "        \n",
    "        return prob, q_emb, a_emb\n",
    "    \n",
    "    def set_train(self):\n",
    "        self.train()\n",
    "        self.bert.train()\n",
    "        # self.bert_a.train()\n",
    "        # self.bert_q.train()\n",
    "        \n",
    "    def set_eval(self):\n",
    "        self.eval()\n",
    "        self.bert.eval()\n",
    "        # self.bert_a.eval()\n",
    "        # self.bert_q.eval()\n",
    "\n",
    "    def get_CLS(self, model, indexed_tokens, attention_mask):\n",
    "        # Map the token strings to their vocabulary indeces.\n",
    "        # indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        # segments_ids = [1] * len(indexed_tokens)\n",
    "        tokens_tensor = indexed_tokens\n",
    "        # segments_tensors = torch.tensor([segments_ids])\n",
    "        outputs = model(tokens_tensor, attention_mask=attention_mask)\n",
    "        assert len(indexed_tokens) == len(attention_mask)\n",
    "        CLS = outputs[0][:,0] # outputs[0][:,0,:]\n",
    "        return CLS\n",
    "    \n",
    "    #Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(self, model, indexed_tokens, attention_mask):\n",
    "        outputs = model(indexed_tokens, attention_mask=attention_mask)\n",
    "        token_embeddings = outputs[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    #Max Pooling - Take the max value over time for every dimension\n",
    "    def max_pooling(self, model, indexed_tokens, attention_mask):\n",
    "        outputs = model(indexed_tokens, attention_mask=attention_mask)\n",
    "        token_embeddings = outputs[0]  #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n",
    "        max_over_time = torch.max(token_embeddings, 1)[0]\n",
    "        return max_over_time\n",
    "    \n",
    "    def get_hidden_state_average(self, indexed_tokens, attention_mask):\n",
    "        # Map the token strings to their vocabulary indeces.\n",
    "        tokens_tensor = indexed_tokens\n",
    "        # segments_tensors = torch.tensor([segments_ids])\n",
    "        outputs = self.bert(tokens_tensor, attention_mask=attention_mask)\n",
    "        # print(outputs.shape)\n",
    "        hidden_state = torch.mean(outputs[0][:,:,:], dim=1)\n",
    "        # print(hidden_state.shape)\n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu = torch.device('cpu')\n",
    "cuda = torch.device('cuda')\n",
    "device = cuda if torch.cuda.is_available() else cpu\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.max_memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_clf = MEDIQA_Model()\n",
    "# bert_clf.load_state_dict(torch.load('models/mediqa_model_clinicalbert_pretrained_entailment_30k'))\n",
    "# bert_clf.load_state_dict(torch.load('checkpoints/model_5_63'))\n",
    "bert_clf = bert_clf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1701, 71960, 234, 1107)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Dataset sizes:')\n",
    "len(train_dataset), len(train_dataset_extended), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22317954419121735"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_extended.y.sum()/len(train_dataset_extended.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def get_test_acc(model, data_loader, true_labels, return_probs_and_labels_and_dists=False):\n",
    "    model.set_eval()\n",
    "    pred_probs, dists = [], []\n",
    "    with torch.no_grad():\n",
    "        for s,r,q,q_att,a,a_att in tqdm.tqdm(data_loader):\n",
    "            logits, emb1, emb2 = model(q.to(device), q_att.to(device),\n",
    "                                 a.to(device), a_att.to(device))\n",
    "            dists.extend(dist(emb1, emb2).to('cpu'))\n",
    "            pred_probs.extend(logits.to('cpu'))\n",
    "        pred_probs  = np.array([x.item() for x in pred_probs])\n",
    "        dists       = np.array([x.item() for x in dists])\n",
    "        pred_labels = (pred_probs > 0.5).astype(np.int16)\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    if return_probs_and_labels_and_dists:\n",
    "        return acc, pred_probs, pred_labels, dists\n",
    "    else:\n",
    "        return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(a,b):\n",
    "    cos = nn.CosineSimilarity(dim=1)\n",
    "    return 1-cos(a,b)\n",
    "    # d = nn.PairwiseDistance()\n",
    "    # return d(a,b)\n",
    "    \n",
    "def ranking_loss(x1, x2, y):\n",
    "    margin = torch.tensor(0.6).to(device)\n",
    "    loss = y*dist(x1,x2) + (1-y)*torch.max(torch.tensor(0.0).to(device), margin - dist(x1,x2))\n",
    "    return torch.mean(loss)\n",
    "\n",
    "def ranking_loss2(x1, x2, y, rank):\n",
    "    def dist(a,b):\n",
    "        cos = nn.CosineSimilarity(dim=1)\n",
    "        return 1-cos(a,b)\n",
    "    margin = torch.tensor(0.60).to(device)\n",
    "    mini_margin = 0.005*(rank-1.0)\n",
    "    loss = y    * (mini_margin - dist(x1,x2)).pow(2) + \\\n",
    "          (1-y) * torch.max(torch.tensor(0.0).to(device), margin - dist(x1,x2))\n",
    "    return torch.mean(loss)\n",
    "\n",
    "def weighted_binary_cross_entropy(output, target, weights=None):\n",
    "    output = torch.clamp(output, 1e-9, 1-1e-9)\n",
    "    if weights is not None:\n",
    "        assert len(weights) == 2\n",
    "        \n",
    "        loss = weights[1] * (target * torch.log(output)) + \\\n",
    "               weights[0] * ((1 - target) * torch.log(1 - output))\n",
    "    else:\n",
    "        loss = target * torch.log(output) + (1 - target) * torch.log(1 - output)\n",
    "    return torch.neg(torch.mean(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in np.array(list(bert_clf.parameters())):\n",
    "#     print(p.shape, p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step 126 0.00179579621180892625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num of steps: 126\n",
      "Epoch 1 mean loss: 0.03206762527143859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.66it/s]\n",
      "100%|██████████| 35/35 [00:52<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.6410256410256411    Val loss:  1.0622353580320365\n",
      "Test acc: 0.5853658536585366   Test loss: 1.2106789246073308\n",
      "\n",
      " step 126 0.00578809296712279356"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num of steps: 126\n",
      "Epoch 2 mean loss: 0.04660908315196052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.64it/s]\n",
      "100%|██████████| 35/35 [00:52<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.6239316239316239    Val loss:  0.8830965803480247\n",
      "Test acc: 0.6097560975609756   Test loss: 1.0041836719038828\n",
      "\n",
      " step 126 6.730578024871647e-053"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num of steps: 126\n",
      "Epoch 3 mean loss: 0.04282593830628068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.63it/s]\n",
      "100%|██████████| 35/35 [00:52<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.6324786324786325    Val loss:  0.9388404236995974\n",
      "Test acc: 0.6124661246612466   Test loss: 1.0459239559121698\n",
      "\n",
      " step 126 0.00165737967472523453"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num of steps: 126\n",
      "Epoch 4 mean loss: 0.04704282483928941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.63it/s]\n",
      "100%|██████████| 35/35 [00:52<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.6282051282051282    Val loss:  0.888671757423518\n",
      "Test acc: 0.6178861788617886   Test loss: 1.0163137836606184\n",
      "\n",
      " step 126 0.0015764976851642132"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num of steps: 126\n",
      "Epoch 5 mean loss: 0.030157375380941796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.61it/s]\n",
      "100%|██████████| 35/35 [00:52<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.6367521367521367    Val loss:  0.8774962611694423\n",
      "Test acc: 0.6133694670280037   Test loss: 1.0079201046621475\n",
      "\n",
      " step 126 0.0040468922816216946"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num of steps: 126\n",
      "Epoch 6 mean loss: 0.01860610243456689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.62it/s]\n",
      "100%|██████████| 35/35 [00:53<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.6410256410256411    Val loss:  0.9146685401115902\n",
      "Test acc: 0.6196928635953026   Test loss: 1.0784316980471564\n",
      "\n",
      " step 126 0.00722386082634329823"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num of steps: 126\n",
      "Epoch 7 mean loss: 0.05730875909889225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.61it/s]\n",
      "100%|██████████| 35/35 [00:52<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.6239316239316239    Val loss:  0.8961528203990357\n",
      "Test acc: 0.6043360433604336   Test loss: 1.143575029933104\n",
      "\n",
      " step 126 0.04655961319804191654"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num of steps: 126\n",
      "Epoch 8 mean loss: 0.027869456307627386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.61it/s]\n",
      "100%|██████████| 35/35 [00:52<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.6025641025641025    Val loss:  0.9678047501986656\n",
      "Test acc: 0.6341463414634146   Test loss: 0.9973962338822789\n",
      "\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "optimizer = torch.optim.Adam(bert_clf.parameters(), lr=8e-6, weight_decay=0.00001)\n",
    "bert_clf.set_train()\n",
    "EPOCHS = 100\n",
    "EARLY_STOPPING = 3\n",
    "\n",
    "lambda1 = 1\n",
    "lambda2 = 1-lambda1\n",
    "loss_func = weighted_binary_cross_entropy\n",
    "\n",
    "train_losses, test_losses, val_losses, val_accs, test_accs = [], [], [], [], []\n",
    "N = len(train_dataset_extended.y)\n",
    "for epoch_num in range(EPOCHS):\n",
    "    losses = []\n",
    "    bert_clf.set_train()\n",
    "    #     loader = train_loader if epoch_num %2 == 0 else train_loader_extended\n",
    "    loader = train_loader_extended\n",
    "    for step_num, batch_data in enumerate(loader):\n",
    "        y_true, rank, questions, q_att, answers, a_att = batch_data #tuple(t.to(device) for t in batch_data)\n",
    "        logits, CLS1, CLS2 = bert_clf(questions.to(device), q_att.to(device),\n",
    "                                      answers.to(device), a_att.to(device))\n",
    "        \n",
    "        # loss = loss_func(logits, y_true.to(device))\n",
    "        # loss = weighted_binary_cross_entropy(logits, y_true.to(device), \n",
    "        #                                              weights=[1,1.85])\n",
    "        loss = lambda1*weighted_binary_cross_entropy(logits, y_true.to(device), \n",
    "                                             weights=[1,3]) + \\\n",
    "               lambda2*ranking_loss2(CLS1, CLS2, y_true.to(device), rank.to(device)) \n",
    "        #  lambda2*ranking_loss(CLS1, CLS2, y_true.to(device))\n",
    "\n",
    "        print(f'\\r step {step_num}', loss.item(), end=\"\")\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        bert_clf.zero_grad()\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        del y_true\n",
    "        del questions\n",
    "        del answers\n",
    "        torch.cuda.empty_cache()\n",
    "        if step_num*BATCH_SIZE > 1000:\n",
    "            break\n",
    "    \n",
    "    print()\n",
    "    print(f'num of steps:', step_num)\n",
    "    print(f'Epoch {epoch_num+1} mean loss:', np.mean(losses))\n",
    "    train_losses.append(np.mean(losses))\n",
    "    val_acc, val_probs_labels, _, _   = get_test_acc(bert_clf, val_loader,  labels_val,  return_probs_and_labels_and_dists=True)\n",
    "    test_acc, test_probs_labels, _, _ = get_test_acc(bert_clf, test_loader, labels_test, return_probs_and_labels_and_dists=True)\n",
    "    val_accs.append(val_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    val_loss  = loss_func(torch.from_numpy(val_probs_labels),  torch.from_numpy(np.array(labels_val, dtype=np.double))).item()\n",
    "    test_loss = loss_func(torch.from_numpy(test_probs_labels), torch.from_numpy(np.array(labels_test, dtype=np.double))).item()\n",
    "    val_losses.append(val_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    print(f'Val acc: ', val_acc, '   Val loss: ', val_loss)\n",
    "    print(f'Test acc:', test_acc, '  Test loss:', test_loss)\n",
    "    print()\n",
    "    \n",
    "    if len(val_accs) <= 1 or val_acc > max(val_accs[:-1]):\n",
    "        torch.save(bert_clf.state_dict(), 'checkpoints/model')\n",
    "    if len(val_losses) > EARLY_STOPPING and val_losses[-(EARLY_STOPPING+1)] < min(val_losses[-EARLY_STOPPING:]):\n",
    "        print('Early stopping')\n",
    "        # recover best execution\n",
    "        model = MEDIQA_Model()\n",
    "        model.load_state_dict(torch.load('checkpoints/model'))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label='train BCE+Rank loss')\n",
    "plt.plot(test_losses, label='test BCE loss')\n",
    "plt.plot(test_accs, label='test acc')\n",
    "plt.plot(val_losses, label='val BCE loss')\n",
    "plt.plot(val_accs, label='val acc')\n",
    "plt.legend()\n",
    "plt.savefig('figures/biobert_loss_extra_data_')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "output_name = 'models/mediqa_model_clinicalbert_best'\n",
    "torch.save(bert_clf.state_dict(), output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "# model = MEDIQA_Model()\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "# model.eval()\n",
    "model = MEDIQA_Model()\n",
    "model.load_state_dict(torch.load('checkpoints/model_6'))\n",
    "# model.load_state_dict(torch.load(output_name))\n",
    "# model.load_state_dict(torch.load('models/mediqa_model_clinicalbert_pretrained_entailment_30k'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:51<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "acc, probs, y_pred, dists = get_test_acc(bert_clf.to(device), test_loader, labels_test, return_probs_and_labels_and_dists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranking_predictions(probs, y):\n",
    "    rankings  = []\n",
    "    entailed = []\n",
    "    i_start  = 0\n",
    "    for i, q in enumerate(QA_test):\n",
    "        rankings.append(1- np.array(probs[i_start:i_start+len(q.answers)]))\n",
    "        entailed.append(y[i_start:i_start+len(q.answers)])\n",
    "        i_start += len(q.answers)\n",
    "        assert len(rankings[i] == len(QA_test[i].answer_ids))\n",
    "        assert len(entailed[i] == len(QA_test[i].answers))\n",
    "    return rankings, entailed\n",
    "\n",
    "def get_ranking_predictions_using_emb_dist(probs, y, dists):\n",
    "    rankings  = []\n",
    "    entailed = []\n",
    "    i_start  = 0\n",
    "    for i, q in enumerate(QA_test):\n",
    "        rankings.append(np.array(dists[i_start:i_start+len(q.answers)]))\n",
    "        entailed.append(y[i_start:i_start+len(q.answers)])\n",
    "        i_start += len(q.answers)\n",
    "        assert len(rankings[i] == len(QA_test[i].answer_ids))\n",
    "        assert len(entailed[i] == len(QA_test[i].answers))\n",
    "    return rankings, entailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = (probs >= 0.50).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22317954419121735"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_dataset_extended.y) / len(train_dataset_extended.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_id,answer_id,label\n"
     ]
    }
   ],
   "source": [
    "ranking_pred, labels_pred = get_ranking_predictions(probs, y_pred_2)\n",
    "# ranking_pred, labels_pred = get_ranking_predictions_using_emb_dist(probs, y_pred_2, dists)\n",
    "csv_name = 'test_biobert_clinicalBERT_CLS_emb_dist_lr25e-4'\n",
    "QA_test.output_predictions(ranking_pred, labels_pred, file=csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Task (Round-2) : 3\n",
      "Ground truth: task3/ground_truth_round_2.csv\n",
      "Submission file: task3/sample_submission_round_2_test_biobert_clinicalBERT_CLS_emb_dist_lr25e-4.csv\n",
      "{'score_acc': 0.6341463414634146, 'score_secondary_spearman': 0.06615463389656936, 'meta': {'MRR': 0.7688888888888886, 'Precision': 0.6680080482897385}}\n"
     ]
    }
   ],
   "source": [
    "evaluate(csv_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "2k-> {'score_acc': 0.5663956639566395, 'score_secondary_spearman': 0.011531841652323562, 'meta': {'MRR': 0.6802222222222222, 'Precision': 0.5954356846473029}}\n",
    "\n",
    "\n",
    "20k-> {'score_acc': 0.6115627822944896, 'score_secondary_spearman': -0.004830917874396123, 'meta': {'MRR': 0.7074444444444444, 'Precision': 0.6898395721925134}}\n",
    "\n",
    "\n",
    "\n",
    "Clinical Mean tokens .25 .75 => {'score_acc': 0.5718157181571816, 'score_secondary_spearman': -0.18549562682215734, 'meta': {'MRR': 0.5188756613756614, 'Precision': 0.5811258278145696}}\n",
    "\n",
    "\n",
    "\n",
    "ClinicalBERT cls 0.4,0.6 --> {'score_acc': 0.5492321589882565, 'score_secondary_spearman': -0.009017857142857147, 'meta': {'MRR': 0.23351587301587295, 'Precision': 0.5714285714285714}}\n",
    "\n",
    "ClinicalBERT cls 0.1,0.9 --> acc: 0.5645889792231256, 'score_secondary_spearman': 0.07665208940719144, 'meta': {'MRR': 0.7538888888888887, 'Precision': 0.5721153846153846\n",
    "\n",
    "ClinicalBERT Mean 0.1,0.9 --> {'score_acc': 0.5582655826558266, 'score_secondary_spearman': 0.08591909882232461, 'meta': {'MRR': 0.3126111111111112, 'Precision': 0.5683690280065898}}\n",
    "\n",
    "WITH LR 1e-6 --> {'score_acc': 0.5754290876242095, spearmans: 0.1446115288220551, 'meta': {'MRR': 0.34247619047619055, 'Precision': 0.5796875}}\n",
    "\n",
    "0.25,0.75 CLS -> {'score_acc': 0.5627822944896116, 'score_secondary_spearman': 0.10020703933747414, 'meta': {'MRR': 0.2872328042328044, 'Precision': 0.5702875399361023}}\n",
    "\n",
    "0.6,0.4, Max pool ->{'score_acc': 0.5709123757904245, 'score_secondary_spearman': 0.20504761904761912, 'meta': {'MRR': 0.2531719576719577, 'Precision': 0.6070640176600441}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BioELMo\n",
    "\n",
    "https://docs.allennlp.org/v1.0.0rc5/tutorials/how_to/elmo/\n",
    "\n",
    "https://github.com/allenai/allennlp/blob/main/allennlp/modules/elmo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install allennlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install allennlp\n",
    "# ! pip install allennlp-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu = torch.device('cpu')\n",
    "cuda = torch.device('cuda')\n",
    "device = cuda if torch.cuda.is_available() else cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gonre\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    }
   ],
   "source": [
    "from allennlp.modules.elmo import Elmo\n",
    "# elmo = Elmo(\n",
    "#     options_file='https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json', \n",
    "#     weight_file='https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5',\n",
    "#     num_output_representations=3,\n",
    "#     dropout=0\n",
    "    \n",
    "# )\n",
    "\n",
    "bioelmo = Elmo(\n",
    "    options_file='bioelmo/biomed_elmo_options.json', \n",
    "    weight_file='bioelmo/biomed_elmo_weights.hdf5',\n",
    "    num_output_representations=3,\n",
    "    dropout=0\n",
    ")\n",
    "bioelmo = bioelmo.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elmo_embedding(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    # print(tokens)\n",
    "    sentences = [tokens]\n",
    "    character_ids = batch_to_ids(sentences).to(device)\n",
    "    return bioelmo(character_ids)['elmo_representations'][2].mean(dim=0).mean(dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1981, -0.2493,  0.1472,  ...,  0.0856,  0.0514,  0.0953],\n",
       "       device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['elmo_representations'][0].mean(dim=0).mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten   = lambda t: [item for sublist in t for item in sublist]\n",
    "\n",
    "sentences = [[q.question, a]  for q in QA for a in q.answers]\n",
    "labels    = flatten([q.labels for q in QA])\n",
    "\n",
    "sentences_extended = [[q.question, a]  for q in QA_extended for a in q.answers]\n",
    "labels_extended    = flatten([q.labels for q in QA_extended])\n",
    "rankings_extended  = flatten([q.reference_rank for q in QA_extended])\n",
    "\n",
    "sentences_val = [[q.question, a]  for q in QA_val for a in q.answers]\n",
    "labels_val    = flatten([q.labels for q in QA_val])\n",
    "\n",
    "sentences_test = [[q.question, a]  for q in QA_test for a in q.answers]\n",
    "labels_test    = flatten([q.labels for q in QA_test]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "BATCH_SIZE_TEST = 64\n",
    "max_len_seq = 512\n",
    "\n",
    "class MEDIQA_Dataset2(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = []\n",
    "        self.y = np.array(y)\n",
    "        for q, a in X:\n",
    "            _q = batch_to_ids([nltk.word_tokenize(q)])\n",
    "            _a = batch_to_ids([nltk.word_tokenize(a)])\n",
    "            self.X.append([_q, _a])\n",
    "        # self.X = np.array(self.X)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        score = train_dataset_extendedsor([self.y[index]])\n",
    "    \n",
    "        q = torch.LongTensor(self.X[index][0])\n",
    "        a = torch.LongTensor(self.X[index][1])\n",
    "        \n",
    "        return score, q, a\n",
    "\n",
    "# Create train dataset\n",
    "train_dataset = MEDIQA_Dataset2(X=sentences, y=labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "LIMIT = 30000\n",
    "train_dataset_extended = MEDIQA_Dataset2(X=sentences_extended[:LIMIT], y=labels_extended[:LIMIT])\n",
    "train_loader_extended = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Create train dataset\n",
    "val_dataset = MEDIQA_Dataset2(X=sentences_val, y=labels_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# Create test dataset\n",
    "test_dataset = MEDIQA_Dataset2(X=sentences_test, y=labels_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE_TEST, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MEDIQA_Model_bioELMo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MEDIQA_Model_bioELMo, self).__init__()\n",
    "        self.bioelmo = Elmo(\n",
    "            options_file='bioelmo/biomed_elmo_options.json', \n",
    "            weight_file='bioelmo/biomed_elmo_weights.hdf5',\n",
    "            num_output_representations=3,\n",
    "            dropout=0\n",
    "        )\n",
    "        for param in self.bioelmo.parameters():\n",
    "            param.requires_grad = False\n",
    "        # self.bioelmo = bioelmo.to(device)\n",
    "        # modules = [self.bert.embeddings, *self.bert.encoder.layer[:-1]] #Replace 5 by what you want\n",
    "        # for module in modules:\n",
    "        #     for param in module.parameters():\n",
    "        #         param.requires_grad = False\n",
    "        self.linear1 = nn.Linear(2*1024, 256)\n",
    "        self.linear2 = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, q, a):\n",
    "        # _, pooled_output = self.bert(tokens, output_all=False)\n",
    "        # print(q.shape, a.shape)\n",
    "        q_emb = self.get_elmo_embedding(q)\n",
    "        a_emb = self.get_elmo_embedding(a)\n",
    "        # print('CLS:', CLS1.shape, CLS2.shape)\n",
    "        x = torch.cat([q_emb, a_emb], dim=0)\n",
    "        # print('concat:', x.shape)\n",
    "        x = F.dropout(x, 0.1)\n",
    "        x = self.linear1(x)\n",
    "        x = nn.SELU()(x)\n",
    "        x = F.dropout(x, 0.2)\n",
    "        x = self.linear2(x)\n",
    "        prob = self.sigmoid(x)\n",
    "        return prob, q_emb, a_emb\n",
    "\n",
    "\n",
    "    def get_elmo_embedding(self, sentence):\n",
    "        #         tokens = nltk.word_tokenize(sentence)\n",
    "        #     print(tokens)\n",
    "        #         sentences = [tokens]\n",
    "        #         character_ids = batch_to_ids(sentence).to(device)\n",
    "        return self.bioelmo(sentence)['elmo_representations'][2].mean(dim=0).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioelmo_clf = MEDIQA_Model_bioELMo()\n",
    "bioelmo_clf = bioelmo_clf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def get_test_acc(model, dataset, true_labels, return_probs_and_labels=False):\n",
    "    model.eval()\n",
    "    pred_probs = []\n",
    "    with torch.no_grad():\n",
    "        for s,(q,a) in tqdm.tqdm(zip(dataset.y, dataset.X)):\n",
    "            logits, _, _ = model(q.to(device),a.to(device))\n",
    "            pred_probs.extend(logits.to('cpu'))\n",
    "        pred_probs  = np.array([x.item() for x in pred_probs])\n",
    "        pred_labels = (pred_probs > 0.5).astype(np.int16)\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    if return_probs_and_labels:\n",
    "        return acc, pred_probs, pred_labels\n",
    "    else:\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-5, weight_decay=0)\n",
    "bert_clf.set_train()\n",
    "EPOCHS = 200\n",
    "EARLY_STOPPING = 10\n",
    "def weighted_binary_cross_entropy(output, target, weights=None):\n",
    "    output = torch.clamp(output, 1e-9, 1-1e-9)\n",
    "    if weights is not None:\n",
    "        assert len(weights) == 2\n",
    "        \n",
    "        loss = weights[1] * (target * torch.log(output)) + \\\n",
    "               weights[0] * ((1 - target) * torch.log(1 - output))\n",
    "    else:\n",
    "        loss = target * torch.log(output) + (1 - target) * torch.log(1 - output)\n",
    "    return torch.neg(torch.mean(loss))\n",
    "lambda1, lambda2 = 0.2, 0.8\n",
    "train_losses, test_losses, val_losses, val_accs, test_accs = [], [], [], [], []\n",
    "for epoch_num in range(EPOCHS):\n",
    "    losses = []\n",
    "    bert_clf.set_train()\n",
    "    loader = train_loader if epoch_num%4==0 else train_loader_extended\n",
    "    for step_num, batch_data in enumerate(train_loader_extended):\n",
    "        y_true, rank, questions, q_att, answers, a_att = batch_data #tuple(t.to(device) for t in batch_data)\n",
    "        if questions.shape != answers.shape: continue\n",
    "        logits, CLS1, CLS2 = bert_clf(questions.to(device), q_att.to(device),\n",
    "                                      answers.to(device), a_att.to(device))\n",
    "        loss_func = weighted_binary_cross_entropy\n",
    "        # pos_weight = torch.ones([len()])*1.8\n",
    "        # loss_func = nn.BCEWithLogitsLoss(pos_weight=1.85)\n",
    "        # loss = loss_func(logits, y_true.to(device))\n",
    "        # loss = weighted_binary_cross_entropy(logits, y_true.to(device), \n",
    "        #                                              weights=[1,1.85])\n",
    "        loss = weighted_binary_cross_entropy(logits, y_true.to(device), \n",
    "                                             weights=[1,1.85])  \n",
    "        # loss = 0.3*ranking_loss2(CLS1, CLS2, y_true.to(device), rank.to(device)) + 0.7*loss_func(logits, y_true.to(device))\n",
    "\n",
    "        print(f'\\r step', loss.item(), end=\"\")\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        bert_clf.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        del y_true\n",
    "        del questions\n",
    "        del answers\n",
    "        torch.cuda.empty_cache()\n",
    "        if step_num > 10000:\n",
    "            break\n",
    "    \n",
    "    print()\n",
    "    print(f'Epoch {epoch_num+1} mean loss:', np.mean(losses))\n",
    "    train_losses.append(np.mean(losses))\n",
    "    val_acc, val_probs_labels, _   = get_test_acc(bert_clf, val_loader,  labels_val,  return_probs_and_labels=True)\n",
    "    test_acc, test_probs_labels, _ = get_test_acc(bert_clf, test_loader, labels_test, return_probs_and_labels=True)\n",
    "    val_accs.append(val_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    val_loss  = loss_func(torch.from_numpy(val_probs_labels),  torch.from_numpy(np.array(labels_val, dtype=np.double))).item()\n",
    "    test_loss = loss_func(torch.from_numpy(test_probs_labels), torch.from_numpy(np.array(labels_test, dtype=np.double))).item()\n",
    "    val_losses.append(val_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    print(f'Val acc: ', val_acc, '   Val loss: ', val_loss)\n",
    "    print(f'Test acc:', test_acc, '  Test loss:', test_loss)\n",
    "    print()\n",
    "    \n",
    "    if len(val_accs) <= 1 or val_acc > max(val_accs[:-1]):\n",
    "        torch.save(bert_clf.state_dict(), 'checkpoints/model')\n",
    "    if len(val_losses) > EARLY_STOPPING and val_losses[-(EARLY_STOPPING+1)] < min(val_losses[-EARLY_STOPPING:]):\n",
    "        print('Early stopping')\n",
    "        # recover best execution\n",
    "        model = MEDIQA_Model()\n",
    "        model.load_state_dict(torch.load('checkpoints/model'))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_binary_cross_entropy(output, target, weights=None):\n",
    "    output = torch.clamp(output, 1e-9, 1-1e-9)\n",
    "    if weights is not None:\n",
    "        assert len(weights) == 2\n",
    "        \n",
    "        loss = weights[1] * (target * torch.log(output)) + \\\n",
    "               weights[0] * ((1 - target) * torch.log(1 - output))\n",
    "    else:\n",
    "        loss = target * torch.log(output) + (1 - target) * torch.log(1 - output)\n",
    "    return torch.neg(torch.mean(loss))\n",
    "lambda1, lambda2 = 0.2, 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gonre\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\torch\\nn\\modules\\container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step 1699/1701 0.677396118640899756563530564308167 0.8110880255699158 0.7263493537902832 0.4915999472141266 0.7726361751556396 0.7660750150680542 1.1248552799224854 1.1601730585098267 0.7556654214859009 0.944179892539978 1.0743881464004517 0.8780591487884521 0.9725364446640015 0.5635057687759399 0.8121901154518127 0.803473174571991 0.5948348045349121 1.8307591676712036 0.9785692095756531 1.14766526222229 0.5109026432037354 0.6453526616096497 0.7687491774559021 0.629677951335907 1.072034478187561 0.36424171924591064 0.5101824402809143 0.42012524604797363 1.0320537090301514 1.092778205871582 1.211172342300415 0.4916948676109314 0.36650553345680237 0.3588463366031647 0.3649001717567444 0.6230849027633667 0.9050266146659851 1.377821683883667 0.831383228302002 0.778199315071106 0.7182212471961975 0.805889904499054 0.3946620523929596 0.5346148014068604 1.0018138885498047 0.5321956872940063 0.888926088809967 1.6900795698165894 0.6829821467399597 0.7774611711502075 0.5511624813079834 0.7035661339759827 1.1213434934616089 0.6978916525840759 1.051118016242981 1.533934235572815 0.9565092325210571 0.6284012198448181 0.9115546941757202 1.0784482955932617 1.110865592956543 0.6374788880348206"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step 1700/1701 1.2094959020614624\n",
      "Epoch 1 mean loss: 0.8490487546438262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "234it [03:21,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.5641025641025641    Val loss:  0.6708293681935693\n",
      "\n",
      " step 4999/1701 0.978369593620300337549312114715576 0.6508950591087341 0.38106876611709595 0.5236749053001404 0.16987085342407227 0.5168029069900513 0.3607616126537323 0.5493770837783813 0.4280032813549042 1.0170741081237793 0.9943052530288696 0.20793093740940094 0.5384392142295837 0.13775594532489777 0.4732307195663452 0.9503716230392456 0.7806406021118164 0.587311863899231 1.9099853038787842 0.15259192883968353 0.6215749382972717 0.25478801131248474 0.4461804926395416 0.27874755859375 0.5270676016807556 0.3104458451271057 0.2987186014652252 0.23476886749267578 0.6333277821540833 0.511217474937439 1.1314187049865723 1.5560166835784912 0.5373008847236633 0.3266569972038269 0.46431952714920044 2.317671060562134 0.38617250323295593 0.31064555048942566 0.2656683623790741 0.2356305718421936 0.244397833943367 0.3600955605506897 2.424513816833496 0.15825973451137543 0.1336822211742401 0.22849348187446594 0.35343295335769653 1.4610919952392578 0.771317720413208 1.3217748403549194 0.5835766792297363 2.3893144130706787 0.36087363958358765 0.2698121964931488 0.5719581246376038 0.6053510308265686 0.5471189618110657 0.6414408087730408 1.4578615427017212 1.8477997779846191 1.4095183610916138 0.4033825397491455 0.7765517830848694 0.6804037094116211 0.15973789989948273 0.3534732758998871 0.7081462144851685 0.6215049028396606 0.38531821966171265 0.19945353269577026 0.3815504312515259 0.39168521761894226 1.7341866493225098 0.4947642683982849 0.5488152503967285 0.22175362706184387 2.277095079421997 2.151721954345703 0.431097149848938 0.765764057636261 0.6051270365715027 1.0194578170776367 1.7953574657440186 1.3425520658493042 0.47531652450561523 1.5887200832366943 0.27847397327423096 0.515869140625 1.7721530199050903 0.5756980180740356 0.800258994102478 1.2980260848999023 1.8505760431289673 1.4379805326461792 1.3170973062515259 0.5994583368301392 0.8909701108932495 0.5818772912025452 0.47307679057121277 0.4608514606952667 0.3690963387489319 2.673288345336914 0.46687740087509155 2.489814281463623 0.5043655633926392 0.6678783893585205 0.46964120864868164 0.35699543356895447 0.37510916590690613 0.33033058047294617 0.2999074161052704 0.3765965402126312 0.35067811608314514 0.28449907898902893 0.3897222578525543 0.20829233527183533 2.3480613231658936 1.8736616373062134 1.7310470342636108 0.5970242023468018 0.5738072395324707 1.611125111579895 0.5650812387466431 0.4092960059642792 1.284109115600586 1.7432435750961304 0.9652066230773926 1.590558409690857 0.5817540287971497 0.8266544342041016 1.426389217376709 0.47486889362335205 0.37035420536994934 0.34703385829925537 0.36445048451423645 0.3550010621547699 0.7400281429290771 0.7648344039916992 2.181025266647339 0.3269883990287781 0.5111210346221924 1.9508730173110962 2.029719114303589 1.016696810722351 0.5382360816001892 0.9158357977867126 2.9446969032287598 2.9095335006713867 0.5894879102706909 0.38283586502075195 2.494032144546509 0.7679603099822998 1.284669041633606 0.48221471905708313 0.7704156637191772 0.5862084627151489 1.220212459564209 0.7595032453536987 1.152571201324463 0.5980204939842224 0.7668115496635437 1.2737480401992798 1.8387653827667236 0.37821194529533386 2.5491771697998047 0.712875485420227 0.42835092544555664 0.6367797255516052 1.7358160018920898 0.43399742245674133 0.3456292152404785 2.097092866897583 0.48203903436660767 1.5894097089767456 0.6896504759788513 0.9801927208900452 0.33227500319480896 0.6334169507026672 0.4271107316017151 0.15726159512996674 0.3335517644882202 0.48299428820610046 0.6713928580284119 0.5985797047615051 0.5596659183502197 0.34507548809051514 0.5862385630607605 0.42849838733673096 2.4067649841308594 2.1955630779266357 0.73930817842483520.30414727330207825 1.9470359086990356 2.148710250854492 0.3252463936805725 0.3096888065338135 1.7000809907913208 0.4406207501888275 0.20538800954818726 2.7237300872802734 0.5251509547233582 2.2139246463775635 0.3080443739891052 0.5022708773612976 1.5237990617752075 0.3418740928173065 0.5498828291893005 0.5731079578399658 2.010150194168091 0.49726998805999756 0.36696842312812805 0.5720543265342712 0.19333522021770477 0.2597869038581848 0.4674248695373535 2.438261032104492 0.3625350594520569 0.20821921527385712 0.34039437770843506 0.38740891218185425 0.3382090628147125 0.2787293493747711 2.256941318511963 1.963926911354065 0.3238714039325714 0.4856429100036621 3.238987445831299 0.4621541202068329 0.28144457936286926 1.2007420063018799 0.6478055715560913 0.8207302689552307 0.47124558687210083 0.4806649088859558 0.40832456946372986 0.5416966080665588 0.24622535705566406 0.3990742564201355 0.2346579134464264 0.28217020630836487 0.5410196781158447 0.5598772764205933"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step 5001/1701 0.5238741636276245\n",
      "Epoch 2 mean loss: 0.8145138185317876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "234it [03:20,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.4658119658119658    Val loss:  0.7035847881919116\n",
      "\n",
      " step 1699/1701 0.685271620750427267218265533447266 1.2643823623657227 0.46548959612846375 0.3872145414352417 0.6765412092208862 0.56010901927948 0.7103338241577148 0.8473151922225952 0.5589075088500977 0.7040688395500183 1.9290854930877686 0.31406962871551514 2.246005058288574 0.8009384274482727 0.5464280843734741 0.9641146659851074 0.5788753628730774 1.4923673868179321 0.4748362898826599 0.5631913542747498 0.5864274501800537 0.9784775972366333 0.5780131220817566 0.4729163646697998 1.2023448944091797 0.602035641670227 0.8031952977180481 1.7431877851486206 0.8035376071929932 0.5615609884262085 1.2399559020996094 2.3189239501953125 0.5871660709381104 0.8578598499298096 1.3722937107086182 1.0283633470535278 0.7585418224334717 0.4985734522342682 0.5534301400184631 1.0583781003952026 0.8253984451293945 0.5058947801589966 0.8449938297271729 0.5195755958557129 2.403000593185425 0.8914840817451477 0.5341424345970154 0.5878429412841797 0.42156001925468445 0.693483829498291 0.6325986385345459 0.1915220469236374 0.6709571480751038 0.7926697731018066 0.3858271837234497 0.8568993210792542 0.46715429425239563 0.13367138803005219 0.7622197270393372 0.5016152858734131 0.37799200415611267 0.49525171518325806 0.21528662741184235"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step 1700/1701 0.2758551239967346\n",
      "Epoch 3 mean loss: 0.812412152928615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "234it [03:20,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.5982905982905983    Val loss:  0.6679818240824085\n",
      "\n",
      " step 4999/1701 0.649899840354919456339498162269592 0.10416968911886215 0.4367392063140869 0.2512764036655426 2.7916111946105957 0.402108758687973 1.8927699327468872 0.9825224876403809 2.614342212677002 0.640494167804718 0.48255276679992676 1.3497859239578247 1.465559959411621 3.013838529586792 0.5561012029647827 0.5942960977554321 0.4190254211425781 0.5826877951622009 0.4930531978607178 0.4533284306526184 0.3882935345172882 1.8271596431732178 1.1111420392990112 0.6902234554290771 1.0261653661727905 1.5251868963241577 0.40500009059906006 0.40768617391586304 1.8599262237548828 0.903292715549469 1.5747833251953125 0.5663719177246094 0.5034537315368652 0.39093974232673645 0.5933812260627747 1.6783947944641113 1.3126311302185059 0.8758187890052795 0.5138370990753174 0.5306258797645569 0.5085223317146301 0.3986878991127014 2.4560718536376953 0.23025523126125336 0.3510036766529083 0.28211748600006104 2.7563796043395996 0.22855527698993683 0.5702741146087646 0.6451030373573303 0.40057265758514404 0.16298995912075043 0.419454425573349 0.5461462736129761 0.5085251331329346 0.4018407464027405 2.673265218734741 0.23451615869998932 0.4412444531917572 0.5355899333953857 0.25060808658599854 0.3983890414237976 0.275867760181427 0.2790135145187378 2.081029176712036 0.8373544216156006 0.722247302532196 0.36584988236427307 0.6338678002357483 2.270251989364624 0.325720876455307 0.4311274290084839 1.953379511833191 0.42616739869117737 0.22691011428833008 0.4896962344646454 0.595875084400177 0.3528602421283722 0.3192857503890991 1.8751877546310425 0.3550282418727875 1.5271011590957642 0.5646698474884033 0.4787192642688751 0.47864529490470886 1.9417445659637451 0.6349716782569885 1.7172962427139282 0.5768731832504272 0.17607299983501434 0.49334463477134705 0.19913111627101898 0.1716390699148178 0.35588228702545166 0.8397130966186523 1.9997694492340088 0.31475383043289185 2.74086332321167 0.15891197323799133 1.932984471321106 0.4953418970108032 0.49773386120796204 0.4203522503376007 0.27669575810432434 0.37120354175567627 0.369613915681839 0.6529345512390137 1.7276349067687988 0.4709312319755554 0.9449306726455688 0.5766202807426453 0.7491683959960938 0.3198457956314087 0.4886648952960968 0.2543196976184845 0.5292692184448242 0.42357519268989563 0.36410823464393616 0.49141374230384827 0.4919884502887726 0.2560686767101288 0.23255598545074463 0.528358519077301 1.864365577697754 0.5675448775291443 0.36817049980163574 0.18600599467754364 0.5882944464683533 0.4389346241950989 0.46744322776794434 0.40804439783096313 1.9498740434646606 0.3770231306552887 0.4024334251880646 0.4508730173110962 0.35188615322113037 0.4308382570743561 0.4901856780052185 0.4148544669151306 0.29638010263442993 0.38787001371383667 0.4014476239681244 1.4296683073043823 0.4976778030395508 0.2819666862487793 0.48173272609710693 0.3564586341381073 0.3897269070148468 0.3893490731716156 0.3623846769332886 0.3427320718765259 1.9311524629592896 0.4961484968662262 1.2655456066131592 0.6842083930969238 0.7162692546844482 1.7406784296035767 2.6232826709747314 2.9977807998657227 0.5338630676269531 0.6251134276390076 0.43984344601631165 0.5061831474304199 1.569967269897461 0.8761167526245117 0.46397507190704346 0.5072486400604248 0.7381070256233215 0.6262221932411194 0.4793480634689331 0.6247130632400513 0.46801692247390747 0.7337451577186584 0.5659173130989075 0.47521376609802246 0.3603329360485077 0.403562068939209 0.4948410987854004 0.579812228679657 0.43484899401664734 2.840452194213867 0.3950599431991577 0.45192378759384155 0.4702775478363037 0.4162813425064087 0.4382361173629761 1.7248601913452148 0.401684433221817 0.46383845806121826 0.75409996509552 0.6840413808822632 0.5625163912773132 0.6146842241287231 1.5847707986831665 0.6919075846672058 1.945770025253296 0.5309814214706421 1.2060116529464722 0.49265986680984497 0.8149215579032898 0.40561771392822266 0.24054059386253357 0.4091361463069916 0.49015259742736816 0.18642008304595947 2.0327088832855225 0.2758861482143402 0.6645103096961975 0.2753903567790985 0.3441060781478882 0.33202365040779114 0.18583787977695465 0.28018954396247864 0.33077389001846313 0.2876299321651459 0.476516455411911 1.0758075714111328 0.27205607295036316 0.5203412771224976 0.39577823877334595 0.4538755714893341 1.0457247495651245 0.35656028985977173 0.5231184959411621 1.7029229402542114 0.537197470664978"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step 5001/1701 2.21498107910156255\n",
      "Epoch 4 mean loss: 0.8094690438823217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "234it [03:20,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.5811965811965812    Val loss:  0.6717089771403743\n",
      "\n",
      " step 1699/1701 0.81218862533569346512054061889648 0.8648598194122314 0.8421000838279724 0.7069045305252075 0.5613862872123718 1.886672019958496 0.7711313366889954 0.8814111948013306 0.32169589400291443 0.5866603851318359 0.9731153845787048 2.1575961112976074 1.2059001922607422 0.6353203058242798 0.08956697583198547 0.3809267282485962 0.6595658659934998 0.6422763466835022 0.2770026922225952 0.955950140953064 0.46008217334747314 0.18766269087791443 0.7201434969902039 1.212383508682251 0.35548579692840576 0.6926416158676147 0.38989052176475525 0.3211022615432739 0.38611900806427 0.3775503933429718 0.3394389748573303 2.4348132610321045 0.8042988181114197 0.5331222414970398 0.5056278705596924 0.4641673266887665 0.5713335871696472 0.7603046298027039 1.6546396017074585 0.33176276087760925 1.2865570783615112 1.2758516073226929 0.37332388758659363 0.3785897195339203 1.0783025026321411 0.5934956073760986 0.4250665605068207 0.7889310121536255 0.6848228573799133 0.5070893168449402 0.7888534665107727 0.5451532602310181 0.5610603094100952 0.5427719354629517 0.8747708797454834 0.33455178141593933 1.6121093034744263 0.24775633215904236 0.11904828995466232 0.8886854648590088 1.1938567161560059 0.46458032727241516"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step 1700/1701 1.3834084272384644\n",
      "Epoch 5 mean loss: 0.7878389497611152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "234it [03:21,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.6068376068376068    Val loss:  0.6614837887041065\n",
      "\n",
      " step 5000/1701 0.812012732028961256756107211112976 0.48432448506355286 0.7096839547157288 0.6860683560371399 0.3536168932914734 0.3315954804420471 0.36044302582740784 0.4092991352081299 0.18900367617607117 0.13163496553897858 0.5993726253509521 0.362749308347702 0.3775375485420227 0.1780773252248764 3.077676296234131 0.1729733943939209 0.4078966975212097 3.2336106300354004 0.42941519618034363 2.038172960281372 0.5709330439567566 0.3276759386062622 0.6697193384170532 0.5464202761650085 0.35624614357948303 0.4701593816280365 1.40386962890625 0.6146262288093567 0.5048670172691345 0.5114908218383789 1.6345075368881226 1.9754830598831177 1.0142216682434082 0.5654856562614441 1.2182960510253906 0.36601540446281433 0.49518197774887085 1.4583227634429932 0.6486642360687256 0.4014047682285309 0.4054821729660034 0.31087327003479004 1.439790964126587 0.3589804768562317 1.6245863437652588 0.39185550808906555 0.360151082277298 0.3342917263507843 0.7588520646095276 0.5330767631530762 2.364414691925049 1.1185356378555298 0.3227464258670807 0.9149589538574219 0.30567964911460876 0.7177769541740417 2.9877214431762695 0.46819746494293213 0.47695428133010864 1.2623423337936401 0.611993134021759 0.5938719511032104 2.631998300552368 2.0210278034210205 0.3194834291934967 0.5118024945259094 0.6935657858848572 0.630404531955719 2.151909589767456 0.2855730950832367 0.3467198610305786 1.1884278059005737 0.7156919240951538 2.125455856323242 0.6361891627311707 0.38461530208587646 0.5004120469093323 0.3376357853412628 1.1092314720153809 0.3372443914413452 0.35130074620246887 0.5099793672561646 0.23884806036949158 1.9952030181884766 0.6137553453445435 0.4152890145778656 2.3594393730163574 1.6565349102020264 2.179283618927002 2.3788068294525146 0.6417735815048218 0.4059845805168152 0.34375718235969543 0.2601723372936249 0.5063040852546692 0.4769410192966461 2.494384527206421 0.5811702013015747 0.6066583395004272 0.6782092452049255 0.2726023197174072 0.4585413634777069 0.24505476653575897 0.4597811698913574 2.3337223529815674 0.35591503977775574 0.3312349021434784 2.019984245300293 0.7033606767654419 0.3489871323108673 0.4475760757923126 1.5057002305984497 0.4354903995990753 0.2736890912055969 0.6100145578384399 1.859207034111023 0.5526736974716187 2.3286149501800537 0.30853211879730225 1.0352232456207275 0.4890461564064026 0.22480855882167816 0.2525782287120819 0.40071678161621094 0.5560423731803894 0.528540313243866 0.6682693362236023 0.47639089822769165 0.2280431091785431 0.9357077479362488 1.8868606090545654 0.41391611099243164 2.884260416030884 2.0378379821777344 0.4788244366645813 1.8556610345840454 1.642337679862976 1.4293861389160156 1.5552983283996582 0.32797831296920776 2.1172142028808594 0.42523816227912903 0.30617624521255493 0.3692689538002014 0.3059021830558777 0.08871960639953613 3.4975221157073975 0.19324056804180145 1.7744390964508057 0.5826292634010315 0.529647946357727 1.7023675441741943 0.5241600275039673 0.5261759757995605 0.5121438503265381 0.5734550356864929 0.3688115179538727 0.6218031644821167 0.24887000024318695 0.7233006358146667 0.6755414009094238 2.3974239826202393 1.169824242591858 1.488869547843933 0.6421876549720764 0.31948229670524597 0.42301762104034424 0.24657592177391052 1.7294865846633911 0.35986053943634033 0.4535895884037018 0.5644518733024597 0.5067583322525024 0.4982618987560272 0.4679070711135864 0.7943415641784668 2.4442665576934814 2.485193967819214 1.4731528759002686 0.7066085934638977 1.8775910139083862 0.38132426142692566 2.244960308074951 0.9148352742195129 3.4468445777893066 0.6418487429618835 0.23859530687332153 0.36841997504234314 1.926132321357727 0.24800468981266022 2.1696743965148926 0.33683058619499207 0.38671162724494934 0.27591583132743835 0.5675902962684631 0.5302850604057312 0.4121254086494446 0.5100004076957703 0.5371785759925842 0.26643985509872437 0.1303187906742096 0.2611658573150635 1.724960207939148 0.13716036081314087 0.13652940094470978 3.1970105171203613 0.2920815050601959 0.2405514419078827 1.6085034608840942 0.9265770316123962 2.7997610569000244 0.2326011061668396 0.27764496207237244 0.30366984009742737 0.36450856924057007 0.9561974406242371 0.4363366365432739 0.33527570962905884 0.463463693857193 0.495180606842041 1.9682077169418335 0.6208177208900452 0.2958530783653259 0.6022595167160034 0.4452483057975769 0.38605058193206787 0.6198247075080872 0.5586720705032349 0.3966914713382721 0.2781735062599182 0.25958341360092163 0.5964846611022949 2.638246536254883 0.29656723141670227 0.3496074676513672 0.17921462655067444 3.1305480003356934 0.38870546221733093 0.3930019438266754 2.1892311573028564 1.3003019094467163 0.4726627469062805"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step 5001/1701 0.6179102659225464\n",
      "Epoch 6 mean loss: 0.8005344001863037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "234it [03:21,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.5384615384615384    Val loss:  0.6981906865132034\n",
      "\n",
      " step 1699/1701 0.0755718871951103252647751569747925 0.5882665514945984 0.3087543249130249 1.8331291675567627 0.5612902641296387 0.6886860728263855 0.4686232805252075 2.252601146697998 0.6688830256462097 1.8626054525375366 0.23918619751930237 0.5696868896484375 0.6189738512039185 0.10290374606847763 3.122542381286621 0.5079092979431152 0.19077354669570923 0.7412747144699097 0.050853144377470016 1.058485984802246 0.7034975290298462 0.09355853497982025 0.37596186995506287 0.04317961260676384 0.9864626526832581 0.45477283000946045 1.3491495847702026 0.4793339967727661 0.36782214045524597 0.8420880436897278 0.5246714949607849 0.4237820506095886 0.38754749298095703 0.502747118473053 0.5409716367721558 0.8814957141876221 0.7063887119293213 0.45977190136909485 1.294333815574646 1.2527137994766235 0.17965266108512878 1.0035353899002075 0.7235183715820312 0.43525615334510803 0.3743114471435547 0.6208179593086243 1.6387603282928467 0.3283330798149109 0.476595938205719 0.0655016228556633 1.096927523612976 0.8396040797233582 0.8938847780227661 0.4387497305870056 0.4141136109828949 1.040250539779663 0.5742080211639404 0.9609408378601074 0.5905700922012329 0.32808199524879456 0.4364340007305145 0.2497812807559967 3.4158480167388916 0.9897092580795288 0.9146308302879333 0.7140657305717468 0.09464016556739807 0.6408706307411194 0.28682905435562134"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step 1700/1701 1.0920313596725464\n",
      "Epoch 7 mean loss: 0.7617821468983168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "234it [03:20,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.6068376068376068    Val loss:  0.6785427533676183\n",
      "\n",
      " step 5000/1701 0.30121946334838867.37568506598472595 0.47752439975738525 2.121912956237793 1.2018150091171265 2.976015329360962 1.224751353263855 0.26194313168525696 2.536450147628784 0.3771395981311798 0.3293743133544922 0.40651607513427734 3.4306414127349854 0.5895897150039673 0.33203819394111633 0.43080341815948486 1.6715726852416992 2.9623303413391113 0.38771164417266846 0.3297651410102844 0.5427716374397278 1.5989000797271729 0.49165576696395874 0.6969074010848999 0.2960631847381592 0.09158382564783096 0.543531596660614 0.7475875020027161 0.25621283054351807 0.3186638653278351 0.242948979139328 0.41045892238616943 0.49786603450775146 0.3640556335449219 0.6634706854820251 2.237905502319336 0.15742957592010498 0.5184263586997986 0.5786778330802917 1.8711354732513428 0.555834949016571 0.17464232444763184 0.5275512933731079 0.19486699998378754 0.5290839672088623 0.28366169333457947 1.550437092781067 0.6974568367004395 2.3178977966308594 0.5635619163513184 1.201184868812561 0.7124068737030029 0.6770067811012268 0.5200229287147522 0.18760278820991516 0.20138730108737946 0.4981059432029724 0.5024515390396118 0.15736573934555054 0.8560062646865845 0.45421862602233887 0.5406801104545593 0.6119558811187744 0.6920608282089233 0.5130623579025269 0.2779814600944519 1.6866209506988525 0.5661035180091858 0.2919454872608185 1.3622753620147705 2.2985663414001465 0.4128323793411255 2.3176751136779785 0.3477698564529419 0.6118952035903931 0.43889689445495605 0.6593393683433533 0.35217583179473877 0.5061761140823364 0.3589726388454437 0.16785405576229095 1.1981258392333984 1.8525794744491577 1.278511881828308 0.4554997980594635 0.6738321781158447 0.6389584541320801 0.3697536885738373 0.3317027986049652 2.241960048675537 0.594318687915802 0.2727627456188202 0.5049405097961426 0.18110960721969604 2.7418630123138428 0.2085420936346054 0.30622708797454834 2.8951098918914795 0.3383122384548187 0.6573262214660645 0.3464342951774597 0.6432881355285645 2.101766586303711 1.6786657571792603 0.45483940839767456 0.44743311405181885 0.15468750894069672 0.6676318645477295 0.5972245335578918 0.9003468155860901 1.5318818092346191 0.46210113167762756 0.439264714717865 1.8129889965057373 0.9265055656433105 0.666697084903717 0.7115328311920166 0.3980535864830017 2.611602306365967 0.44717171788215637 0.43377867341041565 0.4404791593551636 0.5751690864562988 0.4936847984790802 0.43335551023483276 0.6417077779769897 0.5701878666877747 0.2875511944293976 0.5819205045700073 0.3688685894012451 1.7918668985366821 0.4631030559539795 0.5105788707733154 2.3155019283294678 1.3647732734680176 0.3755781650543213 1.651059627532959 1.0663915872573853 2.787193775177002 0.29893526434898376 0.41890352964401245 0.2638452649116516 0.8316270112991333 0.17423322796821594 0.427538126707077 0.4102936089038849 0.6827676892280579 0.26201942563056946 0.4668025076389313 0.3898860514163971 0.2909114956855774 0.4502772092819214 0.5051131248474121 0.18535442650318146 0.28181055188179016 0.838387668132782 0.4455678462982178 0.3719850480556488 0.5822029709815979 0.32257080078125 0.6601133346557617 1.1141018867492676 0.8509093523025513 0.4448751211166382 0.2739195227622986 1.3882206678390503 0.3425644636154175 0.7749834060668945 0.5402423739433289 2.1816604137420654 0.607987105846405 0.2919620871543884 0.28533148765563965 0.47047218680381775 0.6422783732414246 0.6726348400115967 0.6369327306747437 0.6715293526649475 0.5527124404907227 0.3808184564113617 0.4939887523651123 0.5002824664115906 0.4316995441913605 0.730509340763092 1.9614442586898804 1.5753231048583984 0.5181311368942261 0.6982647776603699 2.11637544631958 0.6141365766525269 1.8295388221740723 1.747375726699829 1.5752592086791992 1.4680596590042114 0.65064537525177 0.6208999156951904 0.43855738639831543 0.5217423439025879 0.5532947778701782 0.5166028141975403 0.3815675377845764 0.4040824770927429 0.42865851521492004 0.35644635558128357 2.200824499130249 0.38117530941963196 0.40475356578826904 0.32653388381004333 0.13788196444511414 0.40477991104125977 0.381279319524765 0.6428602337837219 0.5194551944732666 1.2052452564239502 0.3399792015552521 0.40621647238731384 0.48513349890708923 0.44315868616104126 0.3627397119998932 0.40305283665657043 2.025036573410034 0.4080081880092621 0.2502068281173706 0.3742325007915497 0.9808868765830994 1.2279177904129028 2.2417261600494385 0.2949833571910858 0.4305729866027832 0.4803468883037567 0.4132387638092041 0.5744897723197937 0.6777244806289673 1.4869470596313477 0.6202359199523926 0.369019091129303 0.7145938277244568 0.6158803105354309 1.9766604900360107 0.439121276140213 0.500902533531189 0.317492812871933 0.674609899520874 0.4148358702659607 0.4856523871421814 0.23150314390659332 0.5821183919906616 3.17531156539917 0.7315362691879272 0.54084450006484990.6947115063667297 0.3585195541381836 1.1588764190673828 0.4601331353187561 0.1586519479751587 0.5349999070167542 0.4699029326438904 2.4497969150543213 2.1850225925445557 0.28955620527267456 0.43656402826309204 0.1801670491695404 0.32156288623809814 0.37723031640052795 0.2803334891796112 0.38286715745925903 3.2393550872802734 0.3342331051826477 0.32148972153663635 1.9827367067337036 0.9984062910079956 0.34868696331977844 0.902510404586792 0.32610929012298584 2.6000802516937256 1.9991295337677002 0.8304488658905029 1.0006932020187378 0.47593510150909424 1.1728047132492065 0.637650728225708 1.4180461168289185 0.5814493894577026 0.5670967698097229 1.8794591426849365 1.5413755178451538 3.546912908554077 0.28519028425216675 0.351367324590683 0.6407468914985657 0.5454798340797424 0.5338904857635498 0.373553067445755 0.2840639352798462 0.4663674235343933"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step 5001/1701 2.1204919815063477\n",
      "Epoch 8 mean loss: 0.790573516380234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "234it [03:20,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.6282051282051282    Val loss:  0.6593665548079439\n",
      "\n",
      " step 1699/1701 0.48383980989456177638267207145691 0.4401271939277649 2.3578341007232666 1.130565881729126 0.6149922013282776 0.9389269351959229 0.88581383228302 0.5647717118263245 0.6113606095314026 0.7471776604652405 0.808582067489624 0.578160285949707 0.8946181535720825 1.2541364431381226 0.6048128008842468 0.6064196228981018 0.9072501063346863 0.8244301676750183 0.5672140717506409 1.327095627784729 0.4498380422592163 0.6616776585578918 0.47914814949035645 0.3714366853237152 0.6807759404182434 2.4378581047058105 2.801025867462158 0.8124094009399414 0.9950090646743774 1.5024176836013794 0.5719038248062134 0.05102725327014923 0.8338698744773865 0.3271859884262085 0.5346832275390625 0.40009453892707825 0.7169914245605469 0.3188093602657318 0.03933059051632881 0.7563438415527344 0.4779162108898163 0.9931302070617676 0.9396211504936218 0.7646684050559998 0.06638125330209732 0.5484512448310852 0.4507458806037903 1.1538716554641724 0.5490410923957825 0.672971248626709 0.9154461026191711 1.1873961687088013 0.31046491861343384 0.5257895588874817 0.7277061939239502 0.7409461736679077 0.08371711522340775 0.08737834542989731 0.07335302978754044 1.1521201133728027 0.10084298998117447 0.6695327162742615 0.5718930959701538 1.0608501434326172 0.05663116276264191 2.7127811908721924 0.7656995058059692 0.602776050567627 0.16959892213344574 0.5802525281906128 0.1612897664308548 0.7183288931846619 0.4486098289489746 0.1964632123708725"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step 1700/1701 0.5641277432441711\n",
      "Epoch 9 mean loss: 0.7413635242047081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "234it [03:20,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.5170940170940171    Val loss:  0.700332782252205\n",
      "\n",
      " step 5000/1701 0.3693915009498596700895380973816 1.041194200515747 0.84123295545578 2.428391695022583 0.2397446483373642 0.38112330436706543 0.5635817050933838 3.573911428451538 0.57317054271698 0.8722510933876038 0.5508683323860168 0.6560748219490051 0.22927647829055786 0.6535077691078186 0.33846062421798706 0.406705379486084 0.37891262769699097 1.035374641418457 2.4988272190093994 0.7257022261619568 2.2823126316070557 0.3816479444503784 0.28245317935943604 0.5295165777206421 0.20002953708171844 0.9989749193191528 0.4294009804725647 0.48383286595344543 0.2787719666957855 1.7377032041549683 0.6112970113754272 0.3133604824542999 0.5621552467346191 0.7023041844367981 0.20771324634552002 0.6803034543991089 0.360590398311615 0.21888470649719238 1.8210258483886719 0.30211830139160156 2.367882013320923 0.3697977662086487 0.8711736798286438 0.1640927940607071 0.8363257646560669 1.9124997854232788 0.3215293288230896 0.31621667742729187 0.6966196894645691 0.4139133095741272 0.2622889280319214 0.24924924969673157 0.4227420687675476 0.5642626881599426 0.39727646112442017 0.2795077860355377 0.21979326009750366 0.7053751349449158 1.4721710681915283 1.1472012996673584 0.6017393469810486 1.3978469371795654 0.36143139004707336 0.8503278493881226 0.1995500922203064 1.2339533567428589 0.33871376514434814 0.29966288805007935 0.38551461696624756 0.5036141872406006 0.581608772277832 0.22068244218826294 0.1671549528837204 0.3996698260307312 0.3690146207809448 0.775870144367218 3.499908924102783 0.8172733783721924 1.0015575885772705 0.34018856287002563 0.7293580770492554 0.3654254674911499 0.1317562758922577 0.3400209844112396 1.3216345310211182 0.21615855395793915 0.6681264638900757 0.29017728567123413 0.4171076714992523 0.49280914664268494 0.28917473554611206 2.4397289752960205 0.5885423421859741 0.515977144241333 0.4215962886810303 0.1597212553024292 0.48459184169769287 0.3993845283985138 2.1065187454223633 0.32162731885910034 0.19487474858760834 1.1845017671585083 0.42006462812423706 0.2605832815170288 3.2706594467163086 0.48675772547721863 0.6674724221229553 0.2908649444580078 0.25427258014678955 0.8213856816291809 2.0344460010528564 0.18725505471229553 0.2533096671104431 0.6485032439231873 1.8681930303573608 0.5328235626220703 0.3822314441204071 0.6267657279968262 0.16463521122932434 2.2351889610290527 0.5679562091827393 1.9094910621643066 0.18183381855487823 0.2690024673938751 0.3044910430908203 0.5299937725067139 2.5957283973693848 0.29167497158050537 0.519427478313446 0.5080496668815613 0.33642247319221497 0.3422686457633972 0.31823816895484924 2.3009729385375977 0.3554351031780243 1.9036637544631958 0.5051578879356384 2.311372756958008 0.5679174661636353 0.6631033420562744 3.996898651123047 0.4042767584323883 2.686723232269287 0.40149301290512085 0.4665703773498535 2.3330864906311035 0.5256105661392212 1.8699065446853638 2.701042652130127 0.29361864924430847 0.4886478781700134 0.7584648132324219 1.425600528717041 1.1869484186172485 0.7143661975860596 0.47691816091537476 0.47798341512680054 0.5946767330169678 0.5219769477844238 0.3419773578643799 0.4774208962917328 0.5578586459159851 0.4663974344730377 0.688725471496582 0.7315053343772888 1.5415045022964478 0.47367361187934875 2.371971845626831 0.37278109788894653 1.8641554117202759 0.33074551820755005 0.3477860689163208 1.3796265125274658 0.2568933963775635 0.38774001598358154 0.24912506341934204 0.19710370898246765 2.4385859966278076 0.21176952123641968 0.49167323112487793 0.5694653987884521 0.2090519517660141 0.3647240996360779 0.4692171812057495 0.7751966714859009 0.9270166158676147 2.244485855102539 0.26501840353012085 0.5434614419937134 0.344061017036438 0.3039524257183075 0.25755342841148376 0.5374734997749329 0.36074307560920715 1.7833627462387085 1.9199079275131226 0.5817806720733643 0.4929543435573578 0.570320725440979 0.46955493092536926 0.26257556676864624 0.26949766278266907 2.2919790744781494 0.22368109226226807 1.8147788047790527 0.7702469825744629 0.8234847784042358 0.2645273506641388 0.3668488562107086 0.3960854411125183 0.279343843460083 0.13603267073631287 0.4424676299095154 0.4249853193759918 0.26038435101509094 0.6430044174194336 0.32707735896110535 0.38740217685699463 1.8652091026306152 0.4002426266670227 0.3723236918449402 1.3289940357208252 0.8450775146484375 1.0232164859771729 0.7040589451789856"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step 5001/1701 0.5635453462600708\n",
      "Epoch 10 mean loss: 0.7689663434303591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "234it [03:20,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.5427350427350427    Val loss:  0.69562493129673\n",
      "\n",
      " step 1699/1701 1.249252557754516655767752528190613 0.7305711507797241 0.04754018411040306 0.41570696234703064 0.07704950869083405 0.39627182483673096 0.4583096504211426 2.0114264488220215 0.354571670293808 0.7863268852233887 0.33392176032066345 0.7159567475318909 0.6118997931480408 0.7239142656326294 0.5017078518867493 0.6419851183891296 0.4730525314807892 0.30560997128486633 0.2709386348724365 0.7830607891082764 0.5722399353981018 0.6600337028503418 0.10344542562961578 0.32317420840263367 0.11056523025035858 0.46133068203926086 2.40968656539917 1.5269620418548584 0.7609221339225769 1.955933690071106 0.6434159874916077 0.6001956462860107 0.2739569842815399 0.4597349166870117 0.715444803237915 0.27139943838119507 1.105668306350708 0.46638572216033936 1.177466869354248 0.4118368923664093 0.3208426237106323 0.1865958422422409 0.34146708250045776 0.25067248940467834 0.38320600986480713 0.41759297251701355 0.5199999809265137 0.8187353014945984 0.24603857100009918 0.2552158236503601 1.2512409687042236 0.7446568012237549 0.1841553896665573 0.5894473195075989 0.09958329051733017 0.8383573293685913 0.8541109561920166 1.4943839311599731 0.31065067648887634 0.7155585289001465 0.018315063789486885 0.06361579149961472 0.356194406747818 0.5184263586997986 0.39282986521720886 1.3173580169677734 0.5998838543891907 0.41083288192749023 0.7719363570213318"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step 1700/1701 0.6265749931335449\n",
      "Epoch 11 mean loss: 0.728648714244865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "234it [03:21,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.5854700854700855    Val loss:  0.70188399854939\n",
      "\n",
      " step 5000/1701 0.40520256757736206245382508635520935 1.0346301794052124 0.6741978526115417 0.7133738398551941 0.4229075312614441 2.0359344482421875 0.7058670520782471 0.21434152126312256 0.4172942042350769 0.5733693838119507 1.2758562564849854 1.20503830909729 0.5031862854957581 0.3272590637207031 0.2413550168275833 0.24888108670711517 0.2281518280506134 3.1339223384857178 0.2125358283519745 0.3708971440792084 0.48649242520332336 0.5856350064277649 0.16968190670013428 0.58877050876617432.6579999923706055 0.4702901542186737 0.7178834080696106 1.2734766006469727 1.1636031866073608 1.0754306316375732 0.7361448407173157 0.495444118976593 0.6449446678161621 0.8127853870391846 0.24087877571582794 0.5706509947776794 1.321884036064148 1.8261409997940063 1.2642825841903687 0.9220322370529175 0.41048893332481384 0.879149854183197 0.408663272857666 0.504548966884613 3.47416615486145 0.45527195930480957 0.44814199209213257 0.28189682960510254 0.6168897747993469 0.22125786542892456 0.4292804002761841 1.0192198753356934 0.3752056360244751 0.5151129961013794 0.4322254955768585 0.49547433853149414 0.13791440427303314 0.597119927406311 0.5431270599365234 1.5472291707992554 0.7984205484390259 0.3762587010860443 0.7406269907951355 0.29841771721839905 1.4225702285766602 1.0511139631271362 0.6840291023254395 2.12579083442688 0.3032796382904053 0.3914433717727661 2.464311361312866 0.1908201426267624 0.49729254841804504 0.38826891779899597 2.310321092605591 0.21492846310138702 0.34013888239860535 0.49099087715148926 0.3089570701122284 0.308498352766037 0.2390669584274292 0.4522549510002136 0.5720894932746887 0.4311503767967224 0.46195486187934875 1.0989484786987305 2.310666799545288 0.4775683879852295 0.2607857882976532 0.1816217303276062 0.7505332231521606 0.5870735049247742 0.6107237339019775 0.330198734998703 0.2246488630771637 0.598526656627655 1.3177547454833984 0.5421826839447021 1.050154685974121 0.6844373941421509 1.1928958892822266 0.4980403184890747 0.3428679406642914 1.0935101509094238 0.26029542088508606 2.3745009899139404 1.3321141004562378 0.24101203680038452 0.5314739346504211 0.26296743750572205 0.2104736566543579 2.918656349182129 2.453209638595581 0.320003867149353 0.41468533873558044 0.3282063901424408 0.16237004101276398 0.3808761239051819 0.5469810366630554 0.8004773855209351 1.13886296749115 0.2755477726459503 0.33725157380104065 0.35499799251556396 0.44925880432128906 0.48379379510879517 2.827779769897461 0.6816880702972412 2.740264892578125 0.6723759174346924 0.607221782207489 0.9674159288406372 0.8555160164833069 0.46924954652786255 0.2860896587371826 0.5666966438293457 0.43001115322113037 0.6019436120986938 0.19232769310474396 0.6100264191627502 0.2605379521846771 1.1874809265136719 2.8961544036865234 0.5874596834182739 0.5994585752487183 0.34733355045318604 0.29046064615249634 2.7650508880615234 2.2819290161132812 1.7666162252426147 0.7434934973716736 0.4338902533054352 0.2123316526412964 0.4949455261230469 0.3846401572227478 0.1829405426979065 0.34248682856559753 0.44734352827072144 0.5678415298461914 0.2408723384141922 0.8117536902427673 0.5326329469680786 0.27418380975723267 0.3349200487136841 0.342218279838562 0.3710782825946808 1.3673875331878662 0.44368624687194824 0.25687357783317566 0.31467103958129883 0.5442349910736084 0.18356898427009583 0.6665869951248169 0.6532750725746155 0.6035780906677246 1.5799928903579712 0.2631479501724243 0.8382775187492371 1.8069859743118286 0.24704083800315857 0.305738627910614 0.5059857964515686 0.21578647196292877 1.4186073541641235 1.8554898500442505"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step 5001/1701 0.5181573629379272\n",
      "Epoch 12 mean loss: 0.7717251302350442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "234it [03:20,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.6153846153846154    Val loss:  0.7101738785463841\n",
      "\n",
      " step 1699/1701 1.1111154556274414482190866619348526 0.2381696254014969 0.3511202037334442 0.4364570677280426 1.2603425979614258 1.0432521104812622 1.9448961019515991 0.7193167805671692 0.539769172668457 0.24100476503372192 0.5924336314201355 0.017663653939962387 0.7568730115890503 0.7129532694816589 0.6586652994155884 0.4462941884994507 0.9398148655891418 0.4727010130882263 1.6365900039672852 0.8453516364097595 0.7823975086212158 1.0720092058181763 1.1100704669952393 0.7100195288658142 0.5089484453201294 1.1917308568954468 1.8599867820739746 0.7965865135192871 0.5740771889686584 0.19318705797195435 0.3789174258708954 0.2677844762802124 1.0692342519760132 0.35589879751205444 0.755732536315918 0.6182986497879028 0.9169024229049683 0.5583568215370178 0.9561720490455627 0.25511687994003296 0.885118842124939 0.31555238366127014 0.6454349160194397 1.1120638847351074 0.6819559931755066 0.2570134997367859 3.053741455078125 0.5240160822868347 0.7693623900413513 0.5274053812026978 0.9032728672027588 0.0379600003361702 0.9068834781646729 0.49284249544143677 0.48553672432899475"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step 1700/1701 0.19524013996124268\n",
      "Epoch 13 mean loss: 0.7145019330638409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "234it [03:20,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.5982905982905983    Val loss:  0.6855580927564026\n",
      "\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "optimizer = torch.optim.Adam(bioelmo_clf.parameters(), lr=3e-5)\n",
    "bioelmo_clf.train()\n",
    "EPOCHS = 100\n",
    "EARLY_STOPPING = 5\n",
    "loss_func = nn.BCELoss()\n",
    "\n",
    "train_losses, test_losses, val_losses, val_accs, test_accs = [], [], [], [], []\n",
    "N = len(train_dataset.y)\n",
    "for epoch_num in range(EPOCHS):\n",
    "    bioelmo_clf.train()\n",
    "    samples = list(zip(train_dataset.y, train_dataset.X)) if epoch_num%2==0 else list(zip(train_dataset_extended.y, train_dataset_extended.X))\n",
    "    # train_samples = list(zip(train_dataset.y, train_dataset.X))\n",
    "    losses = []\n",
    "    for step_num, batch_data in enumerate(random.sample(samples, len(samples))):\n",
    "        y_true, (questions, answers) = batch_data #tuple(t.to(device) for t in batch_data)\n",
    "        logits, CLS1, CLS2 = bioelmo_clf(questions.to(device), answers.to(device))\n",
    "        y_true = torch.from_numpy(np.array([y_true], dtype=np.float32))\n",
    "        \n",
    "        # loss = loss_func(logits, y_true.to(device))\n",
    "        loss = weighted_binary_cross_entropy(logits, y_true.to(device), \n",
    "                                                     weights=[1,1.85])  \n",
    "\n",
    "        bioelmo_clf.zero_grad()\n",
    "        loss.backward()\n",
    "        print(f'\\r step {step_num}/{N}', loss.item(), end=\"\")\n",
    "        losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        del y_true\n",
    "        del questions\n",
    "        del answers\n",
    "        torch.cuda.empty_cache()\n",
    "        if step_num > 5000:\n",
    "            break\n",
    "        \n",
    "    print()\n",
    "    \n",
    "    print(f'Epoch {epoch_num+1} mean loss:', np.mean(losses))\n",
    "    train_losses.append(np.mean(losses))\n",
    "    val_acc, val_probs_labels, _   = get_test_acc(bioelmo_clf, val_dataset, labels_val,  return_probs_and_labels=True)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    val_loss  = loss_func(torch.from_numpy(val_probs_labels),  torch.from_numpy(np.array(labels_val, dtype=np.double))).item()\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f'Val acc: ', val_acc, '   Val loss: ', val_loss)\n",
    "    print()\n",
    "\n",
    "    \n",
    "\n",
    "    if len(val_accs) <= 1 or val_acc > max(val_accs[:-1]):\n",
    "        torch.save(bioelmo_clf.state_dict(), 'checkpoints/model_elmo')\n",
    "    if len(val_losses) > EARLY_STOPPING and val_losses[-(EARLY_STOPPING+1)] < min(val_losses[-EARLY_STOPPING:]):\n",
    "        print('Early stopping')\n",
    "        # recover best execution\n",
    "        model = MEDIQA_Model_bioELMo()\n",
    "        model.load_state_dict(torch.load('checkpoints/model_elmo'))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(test_losses, label='test loss')\n",
    "plt.plot(test_accs, label='test acc')\n",
    "plt.plot(val_losses, label='val loss')\n",
    "plt.plot(val_accs, label='val acc')\n",
    "plt.legend()\n",
    "plt.savefig('figures/bioelmo_loss_lr_small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1107it [14:48,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "acc, probs, y_pred = get_test_acc(model.to(device), test_dataset, labels_test, return_probs_and_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranking_predictions(probs, y):\n",
    "    rankings  = []\n",
    "    entailed = []\n",
    "    i_start  = 0\n",
    "    for i, q in enumerate(QA_test):\n",
    "        rankings.append(1- np.array(probs[i_start:i_start+len(q.answers)]))\n",
    "        entailed.append(y[i_start:i_start+len(q.answers)])\n",
    "        i_start += len(q.answers)\n",
    "        assert len(rankings[i] == len(QA_test[i].answer_ids))\n",
    "        assert len(entailed[i] == len(QA_test[i].answers))\n",
    "    return rankings, entailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = (probs >= 0.4).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_id,answer_id,label\n",
      "Testing Task (Round-2) : 3\n",
      "Ground truth: task3/ground_truth_round_2.csv\n",
      "Submission file: task3/sample_submission_round_2_test_bioelmo.csv\n",
      "{'score_acc': 0.5329719963866305, 'score_secondary_spearman': 0.07854497354497357, 'meta': {'MRR': 0.5532222222222223, 'Precision': 0.543035993740219}}\n"
     ]
    }
   ],
   "source": [
    "ranking_pred, labels_pred = get_ranking_predictions(probs, y_pred_2)\n",
    "csv_name = 'test_bioelmo'\n",
    "QA_test.output_predictions(ranking_pred, labels_pred, file=csv_name)\n",
    "evaluate(csv_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
